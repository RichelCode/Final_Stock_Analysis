{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61e53ce0-fb7f-4629-8ded-1920541ae685",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_DIR: /notebooks/stock_project\n",
      "PROC_DIR: /notebooks/stock_project/data/processed\n",
      "PRED_DIR: /notebooks/stock_project/reports/predictions\n",
      "TAB_DIR : /notebooks/stock_project/reports/tables\n",
      "FIG_DIR : /notebooks/stock_project/reports/figures\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Global Setup\n",
    "# ==========================\n",
    "import os, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "PROJECT_DIR = Path(\"stock_project\")\n",
    "DATA_DIR = PROJECT_DIR / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROC_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "REPORT_DIR = PROJECT_DIR / \"reports\"\n",
    "PRED_DIR = REPORT_DIR / \"predictions\"\n",
    "TAB_DIR  = REPORT_DIR / \"tables\"\n",
    "FIG_DIR  = REPORT_DIR / \"figures\"\n",
    "MODEL_DIR = PROJECT_DIR / \"models\"\n",
    "\n",
    "for d in [RAW_DIR, PROC_DIR, PRED_DIR, TAB_DIR, FIG_DIR, MODEL_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR.resolve())\n",
    "print(\"PROC_DIR:\", PROC_DIR.resolve())\n",
    "print(\"PRED_DIR:\", PRED_DIR.resolve())\n",
    "print(\"TAB_DIR :\", TAB_DIR.resolve())\n",
    "print(\"FIG_DIR :\", FIG_DIR.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58a03591-1e50-4d85-aa8a-713fd1cc7348",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "panel shape: (19776, 23)\n",
      "tickers: 8\n",
      "split counts:\n",
      " split\n",
      "train    7720\n",
      "val      6048\n",
      "test     6008\n",
      "Name: count, dtype: int64\n",
      "        Date target_date Ticker  target_ret  split\n",
      "0 2016-03-02  2016-03-03   AAPL    0.007417  train\n",
      "1 2016-03-03  2016-03-04   AAPL    0.014767  train\n",
      "2 2016-03-04  2016-03-07   AAPL   -0.011129  train\n",
      "3 2016-03-07  2016-03-08   AAPL   -0.008280  train\n",
      "4 2016-03-08  2016-03-09   AAPL    0.000890  train\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Load Base Panel\n",
    "# ==========================\n",
    "panel_path = PROC_DIR / \"panel.parquet\"\n",
    "assert panel_path.exists(), f\"Missing {panel_path}. Build your panel first.\"\n",
    "\n",
    "panel = pd.read_parquet(panel_path).copy()\n",
    "panel[\"Date\"] = pd.to_datetime(panel[\"Date\"])\n",
    "panel[\"target_date\"] = pd.to_datetime(panel[\"target_date\"])\n",
    "\n",
    "required = [\"Date\",\"target_date\",\"Ticker\",\"split\",\"target_ret\"]\n",
    "missing = [c for c in required if c not in panel.columns]\n",
    "assert not missing, f\"panel missing columns: {missing}\"\n",
    "\n",
    "print(\"panel shape:\", panel.shape)\n",
    "print(\"tickers:\", panel[\"Ticker\"].nunique())\n",
    "print(\"split counts:\\n\", panel[\"split\"].value_counts(dropna=False))\n",
    "print(panel[[\"Date\",\"target_date\",\"Ticker\",\"target_ret\",\"split\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7083ab28-7141-44b4-af02-37208abc1a29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in target_ret: 0\n",
      "target_date min/max by split:\n",
      "              min        max\n",
      "split                      \n",
      "test  2023-01-03 2025-12-30\n",
      "train 2016-03-03 2019-12-31\n",
      "val   2020-01-02 2022-12-30\n",
      "Rows where target_date < Date: 0\n",
      "train_max: 2019-12-31 00:00:00\n",
      "val_min  : 2020-01-02 00:00:00 val_max: 2022-12-30 00:00:00\n",
      "test_min : 2023-01-03 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Integrity & Leakage Checks\n",
    "# ==========================\n",
    "df = panel.copy()\n",
    "\n",
    "# Basic NaN checks\n",
    "print(\"NaNs in target_ret:\", df[\"target_ret\"].isna().sum())\n",
    "\n",
    "# Check split ordering by target_date (train < val < test ideally)\n",
    "df = df.sort_values([\"Ticker\", \"target_date\"])\n",
    "split_minmax = df.groupby(\"split\")[\"target_date\"].agg([\"min\",\"max\"]).sort_index()\n",
    "print(\"target_date min/max by split:\\n\", split_minmax)\n",
    "\n",
    "# Ensure no target_date earlier than Date\n",
    "bad = (df[\"target_date\"] < df[\"Date\"]).sum()\n",
    "print(\"Rows where target_date < Date:\", bad)\n",
    "\n",
    "# Ensure no overlap of target_date between splits (not always strictly true but should usually be)\n",
    "train_max = df.loc[df[\"split\"]==\"train\", \"target_date\"].max()\n",
    "val_min   = df.loc[df[\"split\"]==\"val\", \"target_date\"].min()\n",
    "val_max   = df.loc[df[\"split\"]==\"val\", \"target_date\"].max()\n",
    "test_min  = df.loc[df[\"split\"]==\"test\", \"target_date\"].min()\n",
    "\n",
    "print(\"train_max:\", train_max)\n",
    "print(\"val_min  :\", val_min, \"val_max:\", val_max)\n",
    "print(\"test_min :\", test_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5550e71-268b-4a25-9aa8-e14dee750cc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: regression_metrics, directional_accuracy, oos_r2_vs_baseline_matched\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "#  Metrics Utilities \n",
    "# ==========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def directional_accuracy(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 0.0) -> float:\n",
    "    y_true = np.asarray(y_true).reshape(-1)\n",
    "    y_pred = np.asarray(y_pred).reshape(-1)\n",
    "    return float(np.mean((y_true > eps) == (y_pred > eps)))\n",
    "\n",
    "def regression_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> dict:\n",
    "    y_true = np.asarray(y_true).reshape(-1)\n",
    "    y_pred = np.asarray(y_pred).reshape(-1)\n",
    "\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "    corr = np.nan\n",
    "    if np.std(y_true) > 0 and np.std(y_pred) > 0:\n",
    "        corr = float(np.corrcoef(y_true, y_pred)[0, 1])\n",
    "\n",
    "    dir_acc = directional_accuracy(y_true, y_pred, eps=0.0)\n",
    "\n",
    "    return {\"MAE\": float(mae), \"RMSE\": float(rmse), \"Corr\": corr, \"DirectionalAcc\": float(dir_acc)}\n",
    "\n",
    "def oos_r2_vs_baseline_matched(pred_df: pd.DataFrame, baseline_pred_path: Path) -> dict:\n",
    "    if not baseline_pred_path.exists():\n",
    "        return {s: np.nan for s in [\"train\",\"val\",\"test\"]}\n",
    "\n",
    "    base = pd.read_parquet(baseline_pred_path).copy()\n",
    "    base[\"target_date\"] = pd.to_datetime(base[\"target_date\"])\n",
    "    base = base.rename(columns={\"y_pred\":\"y_pred_baseline\"})[[\"Ticker\",\"target_date\",\"y_pred_baseline\"]]\n",
    "\n",
    "    merged = pred_df.merge(base, on=[\"Ticker\",\"target_date\"], how=\"left\", validate=\"many_to_one\")\n",
    "    out = {}\n",
    "\n",
    "    for split in [\"train\",\"val\",\"test\"]:\n",
    "        d = merged[merged[\"split\"] == split].dropna(subset=[\"y_pred_baseline\"])\n",
    "        if len(d) == 0:\n",
    "            out[split] = np.nan\n",
    "            continue\n",
    "        mse_model = float(np.mean((d[\"y_true\"] - d[\"y_pred\"])**2))\n",
    "        mse_base  = float(np.mean((d[\"y_true\"] - d[\"y_pred_baseline\"])**2))\n",
    "        out[split] = np.nan if mse_base <= 0 else float(1.0 - mse_model/mse_base)\n",
    "    return out\n",
    "\n",
    "print(\"Loaded: regression_metrics, directional_accuracy, oos_r2_vs_baseline_matched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfb402fd-fe49-4e0c-b32e-661f26cd2690",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of numeric features: 18\n",
      "Sample features: ['DFF_diff_lag1', 'DFF_lag1', 'DGS10_diff_lag1', 'DGS10_lag1', 'SP500_lag1', 'mkt_ret_lag1', 'ret', 'ret_lag1', 'ret_lag10', 'ret_lag2', 'ret_lag3', 'ret_lag5', 'ret_vol10', 'ret_vol20', 'ret_vol5', 'sp500_ret_lag1', 'vix_level_lag1', 'vix_ret_lag1']\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "#  Feature Candidates \n",
    "# ==========================\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "NON_FEATURES = {\"Date\",\"target_date\",\"split\",\"Ticker\",\"target_ret\",\"has_garch\"}\n",
    "\n",
    "def numeric_feature_candidates(df: pd.DataFrame) -> list[str]:\n",
    "    feats = []\n",
    "    for c in df.columns:\n",
    "        if c in NON_FEATURES:\n",
    "            continue\n",
    "        if is_numeric_dtype(df[c]):\n",
    "            feats.append(c)\n",
    "    return sorted(feats)\n",
    "\n",
    "feature_candidates_base = numeric_feature_candidates(panel)\n",
    "print(\"Number of numeric features:\", len(feature_candidates_base))\n",
    "print(\"Sample features:\", feature_candidates_base[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7b6ef2-ec91-4e32-9b75-ef759909811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 6 â€” Baseline Mean Models (FULL COPY/PASTE, RUN TOP->BOTTOM)\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "df = panel.copy()\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df[\"target_date\"] = pd.to_datetime(df[\"target_date\"])\n",
    "\n",
    "train_df = df[df[\"split\"]==\"train\"].copy()\n",
    "val_df   = df[df[\"split\"]==\"val\"].copy()\n",
    "test_df  = df[df[\"split\"]==\"test\"].copy()\n",
    "\n",
    "def _save_pred_df(pred_df: pd.DataFrame, model_name: str) -> Path:\n",
    "    pred_df = pred_df.copy()\n",
    "    pred_df[\"target_date\"] = pd.to_datetime(pred_df[\"target_date\"])\n",
    "    path = PRED_DIR / f\"{model_name}.parquet\"\n",
    "    pred_df.to_parquet(path, index=False)\n",
    "    print(f\"Saved predictions: {path} | rows={len(pred_df):,}\")\n",
    "    return path\n",
    "\n",
    "def _metrics_from_pred(pred_df: pd.DataFrame, model_name: str) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for split in [\"train\",\"val\",\"test\"]:\n",
    "        d = pred_df[pred_df[\"split\"]==split]\n",
    "        if len(d)==0:\n",
    "            continue\n",
    "        m = regression_metrics(d[\"y_true\"].to_numpy(), d[\"y_pred\"].to_numpy())\n",
    "        rows.append({\"model\":model_name,\"split\":split,\"n\":int(len(d)),**m})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def _attach_matched_oos_r2(metrics_df: pd.DataFrame, pred_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    baseline_path = PRED_DIR / \"baseline_ticker_mean.parquet\"\n",
    "    oos = oos_r2_vs_baseline_matched(pred_df, baseline_path)\n",
    "    metrics_df = metrics_df.copy()\n",
    "    metrics_df[\"OOS_R2_vs_baseline\"] = metrics_df[\"split\"].map(oos)\n",
    "    return metrics_df\n",
    "\n",
    "def _basic_pred_frame(df_sub: pd.DataFrame, y_pred: np.ndarray, model_name: str) -> pd.DataFrame:\n",
    "    out = df_sub[[\"Date\",\"target_date\",\"Ticker\",\"split\"]].copy()\n",
    "    out[\"model\"] = model_name\n",
    "    out[\"y_true\"] = df_sub[\"target_ret\"].to_numpy()\n",
    "    out[\"y_pred\"] = np.asarray(y_pred).reshape(-1)\n",
    "    out[\"residual\"] = out[\"y_true\"] - out[\"y_pred\"]\n",
    "    return out\n",
    "\n",
    "def _find_ar1_feature(df: pd.DataFrame) -> str:\n",
    "    candidates = [\"ret_lag1\",\"ret_lag_1\",\"ret_l1\",\"lag1_ret\",\"return_lag1\",\"logret_lag1\"]\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    fuzzy = [c for c in df.columns if (\"lag\" in c.lower() and \"ret\" in c.lower() and \"1\" in c)]\n",
    "    if len(fuzzy)>0:\n",
    "        return fuzzy[0]\n",
    "    df[\"ar1_x\"] = df.groupby(\"Ticker\", sort=False)[\"target_ret\"].shift(1)\n",
    "    return \"ar1_x\"\n",
    "\n",
    "all_metrics = []\n",
    "\n",
    "# 6.1 baseline_zero\n",
    "model_name = \"baseline_zero\"\n",
    "pred_df = _basic_pred_frame(df, np.zeros(len(df)), model_name)\n",
    "_save_pred_df(pred_df, model_name)\n",
    "m = _metrics_from_pred(pred_df, model_name)\n",
    "all_metrics.append(m)\n",
    "print(m)\n",
    "\n",
    "# 6.2 baseline_ticker_mean (TRAIN only)\n",
    "model_name = \"baseline_ticker_mean\"\n",
    "ticker_mean = train_df.groupby(\"Ticker\")[\"target_ret\"].mean()\n",
    "global_mean = float(train_df[\"target_ret\"].mean())\n",
    "y_pred = df[\"Ticker\"].map(ticker_mean).fillna(global_mean).to_numpy()\n",
    "pred_df = _basic_pred_frame(df, y_pred, model_name)\n",
    "_save_pred_df(pred_df, model_name)\n",
    "m = _metrics_from_pred(pred_df, model_name)\n",
    "m = _attach_matched_oos_r2(m, pred_df)\n",
    "all_metrics.append(m)\n",
    "print(m)\n",
    "\n",
    "# 6.3 AR1 per ticker\n",
    "model_name = \"ar1_per_ticker\"\n",
    "ar1_col = _find_ar1_feature(df)\n",
    "print(\"AR1 regressor column:\", ar1_col)\n",
    "\n",
    "pred_rows = []\n",
    "for tkr, g in df.sort_values([\"Ticker\",\"Date\"]).groupby(\"Ticker\", sort=False):\n",
    "    g = g.dropna(subset=[ar1_col,\"target_ret\"]).copy()\n",
    "    if len(g) < 50:\n",
    "        yhat = np.full(len(g), float(ticker_mean.get(tkr, global_mean)))\n",
    "        pred_rows.append(_basic_pred_frame(g, yhat, model_name))\n",
    "        continue\n",
    "\n",
    "    g_tr = g[g[\"split\"]==\"train\"]\n",
    "    if len(g_tr) < 30:\n",
    "        yhat = np.full(len(g), float(ticker_mean.get(tkr, global_mean)))\n",
    "        pred_rows.append(_basic_pred_frame(g, yhat, model_name))\n",
    "        continue\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(g_tr[[ar1_col]].to_numpy(), g_tr[\"target_ret\"].to_numpy())\n",
    "    yhat = lr.predict(g[[ar1_col]].to_numpy())\n",
    "    pred_rows.append(_basic_pred_frame(g, yhat, model_name))\n",
    "\n",
    "pred_df = pd.concat(pred_rows, ignore_index=True)\n",
    "_save_pred_df(pred_df, model_name)\n",
    "m = _metrics_from_pred(pred_df, model_name)\n",
    "m = _attach_matched_oos_r2(m, pred_df)\n",
    "all_metrics.append(m)\n",
    "print(m)\n",
    "\n",
    "# 6.4 ridge_pooled\n",
    "model_name = \"ridge_pooled\"\n",
    "num_features = feature_candidates_base.copy()\n",
    "cat_features = [\"Ticker\"]\n",
    "\n",
    "allnan = [c for c in num_features if train_df[c].isna().all()]\n",
    "if allnan:\n",
    "    print(\"Dropping all-NaN train features:\", allnan[:20])\n",
    "    num_features = [c for c in num_features if c not in allnan]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scale\", StandardScaler())\n",
    "        ]), num_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_features)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"prep\", preprocess),\n",
    "    (\"model\", Ridge(alpha=1000.0, random_state=42))\n",
    "])\n",
    "\n",
    "pipe.fit(train_df[num_features + cat_features], train_df[\"target_ret\"])\n",
    "yhat = pipe.predict(df[num_features + cat_features])\n",
    "pred_df = _basic_pred_frame(df, yhat, model_name)\n",
    "_save_pred_df(pred_df, model_name)\n",
    "m = _metrics_from_pred(pred_df, model_name)\n",
    "m = _attach_matched_oos_r2(m, pred_df)\n",
    "all_metrics.append(m)\n",
    "print(m)\n",
    "\n",
    "# 6.5 rf_pooled\n",
    "model_name = \"rf_pooled\"\n",
    "rf_preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", SimpleImputer(strategy=\"median\"), num_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_features)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "rf_pipe = Pipeline([\n",
    "    (\"prep\", rf_preprocess),\n",
    "    (\"model\", RandomForestRegressor(\n",
    "        n_estimators=400,\n",
    "        min_samples_leaf=50,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "rf_pipe.fit(train_df[num_features + cat_features], train_df[\"target_ret\"])\n",
    "yhat = rf_pipe.predict(df[num_features + cat_features])\n",
    "pred_df = _basic_pred_frame(df, yhat, model_name)\n",
    "_save_pred_df(pred_df, model_name)\n",
    "m = _metrics_from_pred(pred_df, model_name)\n",
    "m = _attach_matched_oos_r2(m, pred_df)\n",
    "all_metrics.append(m)\n",
    "print(m)\n",
    "\n",
    "# Save baseline metrics\n",
    "baseline_metrics = pd.concat(all_metrics, ignore_index=True)\n",
    "col_order = [\"model\",\"split\",\"n\",\"MAE\",\"RMSE\",\"Corr\",\"DirectionalAcc\",\"OOS_R2_vs_baseline\"]\n",
    "for c in col_order:\n",
    "    if c not in baseline_metrics.columns:\n",
    "        baseline_metrics[c] = np.nan\n",
    "baseline_metrics = baseline_metrics[col_order].sort_values([\"split\",\"RMSE\"]).reset_index(drop=True)\n",
    "\n",
    "out_path = TAB_DIR / \"baseline_metrics.csv\"\n",
    "baseline_metrics.to_csv(out_path, index=False)\n",
    "print(\"Saved baseline metrics:\", out_path)\n",
    "baseline_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aa6c42-63b4-4375-87a2-c0d24acc860c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
