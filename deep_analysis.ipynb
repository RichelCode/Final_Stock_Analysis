{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61e53ce0-fb7f-4629-8ded-1920541ae685",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_DIR: /notebooks/stock_project\n",
      "PROC_DIR: /notebooks/stock_project/data/processed\n",
      "PRED_DIR: /notebooks/stock_project/reports/predictions\n",
      "TAB_DIR : /notebooks/stock_project/reports/tables\n",
      "FIG_DIR : /notebooks/stock_project/reports/figures\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Global Setup\n",
    "# ==========================\n",
    "import os, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "PROJECT_DIR = Path(\"stock_project\")\n",
    "DATA_DIR = PROJECT_DIR / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROC_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "REPORT_DIR = PROJECT_DIR / \"reports\"\n",
    "PRED_DIR = REPORT_DIR / \"predictions\"\n",
    "TAB_DIR  = REPORT_DIR / \"tables\"\n",
    "FIG_DIR  = REPORT_DIR / \"figures\"\n",
    "MODEL_DIR = PROJECT_DIR / \"models\"\n",
    "\n",
    "for d in [RAW_DIR, PROC_DIR, PRED_DIR, TAB_DIR, FIG_DIR, MODEL_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR.resolve())\n",
    "print(\"PROC_DIR:\", PROC_DIR.resolve())\n",
    "print(\"PRED_DIR:\", PRED_DIR.resolve())\n",
    "print(\"TAB_DIR :\", TAB_DIR.resolve())\n",
    "print(\"FIG_DIR :\", FIG_DIR.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58a03591-1e50-4d85-aa8a-713fd1cc7348",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "panel shape: (19776, 23)\n",
      "tickers: 8\n",
      "split counts:\n",
      " split\n",
      "train    7720\n",
      "val      6048\n",
      "test     6008\n",
      "Name: count, dtype: int64\n",
      "        Date target_date Ticker  target_ret  split\n",
      "0 2016-03-02  2016-03-03   AAPL    0.007417  train\n",
      "1 2016-03-03  2016-03-04   AAPL    0.014767  train\n",
      "2 2016-03-04  2016-03-07   AAPL   -0.011129  train\n",
      "3 2016-03-07  2016-03-08   AAPL   -0.008280  train\n",
      "4 2016-03-08  2016-03-09   AAPL    0.000890  train\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Load Base Panel\n",
    "# ==========================\n",
    "panel_path = PROC_DIR / \"panel.parquet\"\n",
    "assert panel_path.exists(), f\"Missing {panel_path}. Build your panel first.\"\n",
    "\n",
    "panel = pd.read_parquet(panel_path).copy()\n",
    "panel[\"Date\"] = pd.to_datetime(panel[\"Date\"])\n",
    "panel[\"target_date\"] = pd.to_datetime(panel[\"target_date\"])\n",
    "\n",
    "required = [\"Date\",\"target_date\",\"Ticker\",\"split\",\"target_ret\"]\n",
    "missing = [c for c in required if c not in panel.columns]\n",
    "assert not missing, f\"panel missing columns: {missing}\"\n",
    "\n",
    "print(\"panel shape:\", panel.shape)\n",
    "print(\"tickers:\", panel[\"Ticker\"].nunique())\n",
    "print(\"split counts:\\n\", panel[\"split\"].value_counts(dropna=False))\n",
    "print(panel[[\"Date\",\"target_date\",\"Ticker\",\"target_ret\",\"split\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7083ab28-7141-44b4-af02-37208abc1a29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in target_ret: 0\n",
      "target_date min/max by split:\n",
      "              min        max\n",
      "split                      \n",
      "test  2023-01-03 2025-12-30\n",
      "train 2016-03-03 2019-12-31\n",
      "val   2020-01-02 2022-12-30\n",
      "Rows where target_date < Date: 0\n",
      "train_max: 2019-12-31 00:00:00\n",
      "val_min  : 2020-01-02 00:00:00 val_max: 2022-12-30 00:00:00\n",
      "test_min : 2023-01-03 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Integrity & Leakage Checks\n",
    "# ==========================\n",
    "df = panel.copy()\n",
    "\n",
    "# Basic NaN checks\n",
    "print(\"NaNs in target_ret:\", df[\"target_ret\"].isna().sum())\n",
    "\n",
    "# Check split ordering by target_date (train < val < test ideally)\n",
    "df = df.sort_values([\"Ticker\", \"target_date\"])\n",
    "split_minmax = df.groupby(\"split\")[\"target_date\"].agg([\"min\",\"max\"]).sort_index()\n",
    "print(\"target_date min/max by split:\\n\", split_minmax)\n",
    "\n",
    "# Ensure no target_date earlier than Date\n",
    "bad = (df[\"target_date\"] < df[\"Date\"]).sum()\n",
    "print(\"Rows where target_date < Date:\", bad)\n",
    "\n",
    "# Ensure no overlap of target_date between splits (not always strictly true but should usually be)\n",
    "train_max = df.loc[df[\"split\"]==\"train\", \"target_date\"].max()\n",
    "val_min   = df.loc[df[\"split\"]==\"val\", \"target_date\"].min()\n",
    "val_max   = df.loc[df[\"split\"]==\"val\", \"target_date\"].max()\n",
    "test_min  = df.loc[df[\"split\"]==\"test\", \"target_date\"].min()\n",
    "\n",
    "print(\"train_max:\", train_max)\n",
    "print(\"val_min  :\", val_min, \"val_max:\", val_max)\n",
    "print(\"test_min :\", test_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5550e71-268b-4a25-9aa8-e14dee750cc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: regression_metrics, directional_accuracy, oos_r2_vs_baseline_matched\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "#  Metrics Utilities \n",
    "# ==========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def directional_accuracy(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 0.0) -> float:\n",
    "    y_true = np.asarray(y_true).reshape(-1)\n",
    "    y_pred = np.asarray(y_pred).reshape(-1)\n",
    "    return float(np.mean((y_true > eps) == (y_pred > eps)))\n",
    "\n",
    "def regression_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> dict:\n",
    "    y_true = np.asarray(y_true).reshape(-1)\n",
    "    y_pred = np.asarray(y_pred).reshape(-1)\n",
    "\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "    corr = np.nan\n",
    "    if np.std(y_true) > 0 and np.std(y_pred) > 0:\n",
    "        corr = float(np.corrcoef(y_true, y_pred)[0, 1])\n",
    "\n",
    "    dir_acc = directional_accuracy(y_true, y_pred, eps=0.0)\n",
    "\n",
    "    return {\"MAE\": float(mae), \"RMSE\": float(rmse), \"Corr\": corr, \"DirectionalAcc\": float(dir_acc)}\n",
    "\n",
    "def oos_r2_vs_baseline_matched(pred_df: pd.DataFrame, baseline_pred_path: Path) -> dict:\n",
    "    if not baseline_pred_path.exists():\n",
    "        return {s: np.nan for s in [\"train\",\"val\",\"test\"]}\n",
    "\n",
    "    base = pd.read_parquet(baseline_pred_path).copy()\n",
    "    base[\"target_date\"] = pd.to_datetime(base[\"target_date\"])\n",
    "    base = base.rename(columns={\"y_pred\":\"y_pred_baseline\"})[[\"Ticker\",\"target_date\",\"y_pred_baseline\"]]\n",
    "\n",
    "    merged = pred_df.merge(base, on=[\"Ticker\",\"target_date\"], how=\"left\", validate=\"many_to_one\")\n",
    "    out = {}\n",
    "\n",
    "    for split in [\"train\",\"val\",\"test\"]:\n",
    "        d = merged[merged[\"split\"] == split].dropna(subset=[\"y_pred_baseline\"])\n",
    "        if len(d) == 0:\n",
    "            out[split] = np.nan\n",
    "            continue\n",
    "        mse_model = float(np.mean((d[\"y_true\"] - d[\"y_pred\"])**2))\n",
    "        mse_base  = float(np.mean((d[\"y_true\"] - d[\"y_pred_baseline\"])**2))\n",
    "        out[split] = np.nan if mse_base <= 0 else float(1.0 - mse_model/mse_base)\n",
    "    return out\n",
    "\n",
    "print(\"Loaded: regression_metrics, directional_accuracy, oos_r2_vs_baseline_matched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfb402fd-fe49-4e0c-b32e-661f26cd2690",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of numeric features: 18\n",
      "Sample features: ['DFF_diff_lag1', 'DFF_lag1', 'DGS10_diff_lag1', 'DGS10_lag1', 'SP500_lag1', 'mkt_ret_lag1', 'ret', 'ret_lag1', 'ret_lag10', 'ret_lag2', 'ret_lag3', 'ret_lag5', 'ret_vol10', 'ret_vol20', 'ret_vol5', 'sp500_ret_lag1', 'vix_level_lag1', 'vix_ret_lag1']\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "#  Feature Candidates \n",
    "# ==========================\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "NON_FEATURES = {\"Date\",\"target_date\",\"split\",\"Ticker\",\"target_ret\",\"has_garch\"}\n",
    "\n",
    "def numeric_feature_candidates(df: pd.DataFrame) -> list[str]:\n",
    "    feats = []\n",
    "    for c in df.columns:\n",
    "        if c in NON_FEATURES:\n",
    "            continue\n",
    "        if is_numeric_dtype(df[c]):\n",
    "            feats.append(c)\n",
    "    return sorted(feats)\n",
    "\n",
    "feature_candidates_base = numeric_feature_candidates(panel)\n",
    "print(\"Number of numeric features:\", len(feature_candidates_base))\n",
    "print(\"Sample features:\", feature_candidates_base[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c7b6ef2-ec91-4e32-9b75-ef759909811f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions: stock_project/reports/predictions/baseline_zero.parquet | rows=19,776\n",
      "           model  split     n       MAE      RMSE  Corr  DirectionalAcc\n",
      "0  baseline_zero  train  7720  0.012403  0.019210   NaN        0.461140\n",
      "1  baseline_zero    val  6048  0.019788  0.029259   NaN        0.477844\n",
      "2  baseline_zero   test  6008  0.016531  0.024833   NaN        0.457390\n",
      "Saved predictions: stock_project/reports/predictions/baseline_ticker_mean.parquet | rows=19,776\n",
      "                  model  split     n       MAE      RMSE      Corr  \\\n",
      "0  baseline_ticker_mean  train  7720  0.012353  0.019176  0.025203   \n",
      "1  baseline_ticker_mean    val  6048  0.019762  0.029256  0.007244   \n",
      "2  baseline_ticker_mean   test  6008  0.016471  0.024781  0.015846   \n",
      "\n",
      "   DirectionalAcc  OOS_R2_vs_baseline  \n",
      "0        0.538860                 0.0  \n",
      "1        0.522156                 0.0  \n",
      "2        0.542610                 0.0  \n",
      "AR1 regressor column: ret_lag1\n",
      "Saved predictions: stock_project/reports/predictions/ar1_per_ticker.parquet | rows=19,776\n",
      "            model  split     n       MAE      RMSE      Corr  DirectionalAcc  \\\n",
      "0  ar1_per_ticker  train  7720  0.012353  0.019163  0.044651        0.536269   \n",
      "1  ar1_per_ticker    val  6048  0.019799  0.029304 -0.018409        0.514054   \n",
      "2  ar1_per_ticker   test  6008  0.016473  0.024786  0.020092        0.542610   \n",
      "\n",
      "   OOS_R2_vs_baseline  \n",
      "0            0.001359  \n",
      "1           -0.003284  \n",
      "2           -0.000373  \n",
      "Saved predictions: stock_project/reports/predictions/ridge_pooled.parquet | rows=19,776\n",
      "          model  split     n       MAE      RMSE      Corr  DirectionalAcc  \\\n",
      "0  ridge_pooled  train  7720  0.012329  0.019092  0.098605        0.540026   \n",
      "1  ridge_pooled    val  6048  0.019867  0.029437 -0.006374        0.527943   \n",
      "2  ridge_pooled   test  6008  0.017224  0.025323  0.039794        0.460220   \n",
      "\n",
      "   OOS_R2_vs_baseline  \n",
      "0            0.008793  \n",
      "1           -0.012363  \n",
      "2           -0.044188  \n",
      "Saved predictions: stock_project/reports/predictions/rf_pooled.parquet | rows=19,776\n",
      "       model  split     n       MAE      RMSE      Corr  DirectionalAcc  \\\n",
      "0  rf_pooled  train  7720  0.011018  0.017436  0.580693        0.704793   \n",
      "1  rf_pooled    val  6048  0.020011  0.029469  0.013791        0.503638   \n",
      "2  rf_pooled   test  6008  0.019257  0.027006  0.045119        0.462051   \n",
      "\n",
      "   OOS_R2_vs_baseline  \n",
      "0            0.173290  \n",
      "1           -0.014568  \n",
      "2           -0.187616  \n",
      "Saved baseline metrics: stock_project/reports/tables/baseline_metrics.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>split</th>\n",
       "      <th>n</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Corr</th>\n",
       "      <th>DirectionalAcc</th>\n",
       "      <th>OOS_R2_vs_baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline_ticker_mean</td>\n",
       "      <td>test</td>\n",
       "      <td>6008</td>\n",
       "      <td>0.016471</td>\n",
       "      <td>0.024781</td>\n",
       "      <td>0.015846</td>\n",
       "      <td>0.542610</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ar1_per_ticker</td>\n",
       "      <td>test</td>\n",
       "      <td>6008</td>\n",
       "      <td>0.016473</td>\n",
       "      <td>0.024786</td>\n",
       "      <td>0.020092</td>\n",
       "      <td>0.542610</td>\n",
       "      <td>-0.000373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>baseline_zero</td>\n",
       "      <td>test</td>\n",
       "      <td>6008</td>\n",
       "      <td>0.016531</td>\n",
       "      <td>0.024833</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.457390</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ridge_pooled</td>\n",
       "      <td>test</td>\n",
       "      <td>6008</td>\n",
       "      <td>0.017224</td>\n",
       "      <td>0.025323</td>\n",
       "      <td>0.039794</td>\n",
       "      <td>0.460220</td>\n",
       "      <td>-0.044188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rf_pooled</td>\n",
       "      <td>test</td>\n",
       "      <td>6008</td>\n",
       "      <td>0.019257</td>\n",
       "      <td>0.027006</td>\n",
       "      <td>0.045119</td>\n",
       "      <td>0.462051</td>\n",
       "      <td>-0.187616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rf_pooled</td>\n",
       "      <td>train</td>\n",
       "      <td>7720</td>\n",
       "      <td>0.011018</td>\n",
       "      <td>0.017436</td>\n",
       "      <td>0.580693</td>\n",
       "      <td>0.704793</td>\n",
       "      <td>0.173290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ridge_pooled</td>\n",
       "      <td>train</td>\n",
       "      <td>7720</td>\n",
       "      <td>0.012329</td>\n",
       "      <td>0.019092</td>\n",
       "      <td>0.098605</td>\n",
       "      <td>0.540026</td>\n",
       "      <td>0.008793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ar1_per_ticker</td>\n",
       "      <td>train</td>\n",
       "      <td>7720</td>\n",
       "      <td>0.012353</td>\n",
       "      <td>0.019163</td>\n",
       "      <td>0.044651</td>\n",
       "      <td>0.536269</td>\n",
       "      <td>0.001359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>baseline_ticker_mean</td>\n",
       "      <td>train</td>\n",
       "      <td>7720</td>\n",
       "      <td>0.012353</td>\n",
       "      <td>0.019176</td>\n",
       "      <td>0.025203</td>\n",
       "      <td>0.538860</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>baseline_zero</td>\n",
       "      <td>train</td>\n",
       "      <td>7720</td>\n",
       "      <td>0.012403</td>\n",
       "      <td>0.019210</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.461140</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>baseline_ticker_mean</td>\n",
       "      <td>val</td>\n",
       "      <td>6048</td>\n",
       "      <td>0.019762</td>\n",
       "      <td>0.029256</td>\n",
       "      <td>0.007244</td>\n",
       "      <td>0.522156</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>baseline_zero</td>\n",
       "      <td>val</td>\n",
       "      <td>6048</td>\n",
       "      <td>0.019788</td>\n",
       "      <td>0.029259</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.477844</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ar1_per_ticker</td>\n",
       "      <td>val</td>\n",
       "      <td>6048</td>\n",
       "      <td>0.019799</td>\n",
       "      <td>0.029304</td>\n",
       "      <td>-0.018409</td>\n",
       "      <td>0.514054</td>\n",
       "      <td>-0.003284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ridge_pooled</td>\n",
       "      <td>val</td>\n",
       "      <td>6048</td>\n",
       "      <td>0.019867</td>\n",
       "      <td>0.029437</td>\n",
       "      <td>-0.006374</td>\n",
       "      <td>0.527943</td>\n",
       "      <td>-0.012363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rf_pooled</td>\n",
       "      <td>val</td>\n",
       "      <td>6048</td>\n",
       "      <td>0.020011</td>\n",
       "      <td>0.029469</td>\n",
       "      <td>0.013791</td>\n",
       "      <td>0.503638</td>\n",
       "      <td>-0.014568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   model  split     n       MAE      RMSE      Corr  \\\n",
       "0   baseline_ticker_mean   test  6008  0.016471  0.024781  0.015846   \n",
       "1         ar1_per_ticker   test  6008  0.016473  0.024786  0.020092   \n",
       "2          baseline_zero   test  6008  0.016531  0.024833       NaN   \n",
       "3           ridge_pooled   test  6008  0.017224  0.025323  0.039794   \n",
       "4              rf_pooled   test  6008  0.019257  0.027006  0.045119   \n",
       "5              rf_pooled  train  7720  0.011018  0.017436  0.580693   \n",
       "6           ridge_pooled  train  7720  0.012329  0.019092  0.098605   \n",
       "7         ar1_per_ticker  train  7720  0.012353  0.019163  0.044651   \n",
       "8   baseline_ticker_mean  train  7720  0.012353  0.019176  0.025203   \n",
       "9          baseline_zero  train  7720  0.012403  0.019210       NaN   \n",
       "10  baseline_ticker_mean    val  6048  0.019762  0.029256  0.007244   \n",
       "11         baseline_zero    val  6048  0.019788  0.029259       NaN   \n",
       "12        ar1_per_ticker    val  6048  0.019799  0.029304 -0.018409   \n",
       "13          ridge_pooled    val  6048  0.019867  0.029437 -0.006374   \n",
       "14             rf_pooled    val  6048  0.020011  0.029469  0.013791   \n",
       "\n",
       "    DirectionalAcc  OOS_R2_vs_baseline  \n",
       "0         0.542610            0.000000  \n",
       "1         0.542610           -0.000373  \n",
       "2         0.457390                 NaN  \n",
       "3         0.460220           -0.044188  \n",
       "4         0.462051           -0.187616  \n",
       "5         0.704793            0.173290  \n",
       "6         0.540026            0.008793  \n",
       "7         0.536269            0.001359  \n",
       "8         0.538860            0.000000  \n",
       "9         0.461140                 NaN  \n",
       "10        0.522156            0.000000  \n",
       "11        0.477844                 NaN  \n",
       "12        0.514054           -0.003284  \n",
       "13        0.527943           -0.012363  \n",
       "14        0.503638           -0.014568  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Baseline Mean Models (FULL COPY/PASTE, RUN TOP->BOTTOM)\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "df = panel.copy()\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df[\"target_date\"] = pd.to_datetime(df[\"target_date\"])\n",
    "\n",
    "train_df = df[df[\"split\"]==\"train\"].copy()\n",
    "val_df   = df[df[\"split\"]==\"val\"].copy()\n",
    "test_df  = df[df[\"split\"]==\"test\"].copy()\n",
    "\n",
    "def _save_pred_df(pred_df: pd.DataFrame, model_name: str) -> Path:\n",
    "    pred_df = pred_df.copy()\n",
    "    pred_df[\"target_date\"] = pd.to_datetime(pred_df[\"target_date\"])\n",
    "    path = PRED_DIR / f\"{model_name}.parquet\"\n",
    "    pred_df.to_parquet(path, index=False)\n",
    "    print(f\"Saved predictions: {path} | rows={len(pred_df):,}\")\n",
    "    return path\n",
    "\n",
    "def _metrics_from_pred(pred_df: pd.DataFrame, model_name: str) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for split in [\"train\",\"val\",\"test\"]:\n",
    "        d = pred_df[pred_df[\"split\"]==split]\n",
    "        if len(d)==0:\n",
    "            continue\n",
    "        m = regression_metrics(d[\"y_true\"].to_numpy(), d[\"y_pred\"].to_numpy())\n",
    "        rows.append({\"model\":model_name,\"split\":split,\"n\":int(len(d)),**m})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def _attach_matched_oos_r2(metrics_df: pd.DataFrame, pred_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    baseline_path = PRED_DIR / \"baseline_ticker_mean.parquet\"\n",
    "    oos = oos_r2_vs_baseline_matched(pred_df, baseline_path)\n",
    "    metrics_df = metrics_df.copy()\n",
    "    metrics_df[\"OOS_R2_vs_baseline\"] = metrics_df[\"split\"].map(oos)\n",
    "    return metrics_df\n",
    "\n",
    "def _basic_pred_frame(df_sub: pd.DataFrame, y_pred: np.ndarray, model_name: str) -> pd.DataFrame:\n",
    "    out = df_sub[[\"Date\",\"target_date\",\"Ticker\",\"split\"]].copy()\n",
    "    out[\"model\"] = model_name\n",
    "    out[\"y_true\"] = df_sub[\"target_ret\"].to_numpy()\n",
    "    out[\"y_pred\"] = np.asarray(y_pred).reshape(-1)\n",
    "    out[\"residual\"] = out[\"y_true\"] - out[\"y_pred\"]\n",
    "    return out\n",
    "\n",
    "def _find_ar1_feature(df: pd.DataFrame) -> str:\n",
    "    candidates = [\"ret_lag1\",\"ret_lag_1\",\"ret_l1\",\"lag1_ret\",\"return_lag1\",\"logret_lag1\"]\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    fuzzy = [c for c in df.columns if (\"lag\" in c.lower() and \"ret\" in c.lower() and \"1\" in c)]\n",
    "    if len(fuzzy)>0:\n",
    "        return fuzzy[0]\n",
    "    df[\"ar1_x\"] = df.groupby(\"Ticker\", sort=False)[\"target_ret\"].shift(1)\n",
    "    return \"ar1_x\"\n",
    "\n",
    "all_metrics = []\n",
    "\n",
    "# 6.1 baseline_zero\n",
    "model_name = \"baseline_zero\"\n",
    "pred_df = _basic_pred_frame(df, np.zeros(len(df)), model_name)\n",
    "_save_pred_df(pred_df, model_name)\n",
    "m = _metrics_from_pred(pred_df, model_name)\n",
    "all_metrics.append(m)\n",
    "print(m)\n",
    "\n",
    "# 6.2 baseline_ticker_mean (TRAIN only)\n",
    "model_name = \"baseline_ticker_mean\"\n",
    "ticker_mean = train_df.groupby(\"Ticker\")[\"target_ret\"].mean()\n",
    "global_mean = float(train_df[\"target_ret\"].mean())\n",
    "y_pred = df[\"Ticker\"].map(ticker_mean).fillna(global_mean).to_numpy()\n",
    "pred_df = _basic_pred_frame(df, y_pred, model_name)\n",
    "_save_pred_df(pred_df, model_name)\n",
    "m = _metrics_from_pred(pred_df, model_name)\n",
    "m = _attach_matched_oos_r2(m, pred_df)\n",
    "all_metrics.append(m)\n",
    "print(m)\n",
    "\n",
    "# 6.3 AR1 per ticker\n",
    "model_name = \"ar1_per_ticker\"\n",
    "ar1_col = _find_ar1_feature(df)\n",
    "print(\"AR1 regressor column:\", ar1_col)\n",
    "\n",
    "pred_rows = []\n",
    "for tkr, g in df.sort_values([\"Ticker\",\"Date\"]).groupby(\"Ticker\", sort=False):\n",
    "    g = g.dropna(subset=[ar1_col,\"target_ret\"]).copy()\n",
    "    if len(g) < 50:\n",
    "        yhat = np.full(len(g), float(ticker_mean.get(tkr, global_mean)))\n",
    "        pred_rows.append(_basic_pred_frame(g, yhat, model_name))\n",
    "        continue\n",
    "\n",
    "    g_tr = g[g[\"split\"]==\"train\"]\n",
    "    if len(g_tr) < 30:\n",
    "        yhat = np.full(len(g), float(ticker_mean.get(tkr, global_mean)))\n",
    "        pred_rows.append(_basic_pred_frame(g, yhat, model_name))\n",
    "        continue\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(g_tr[[ar1_col]].to_numpy(), g_tr[\"target_ret\"].to_numpy())\n",
    "    yhat = lr.predict(g[[ar1_col]].to_numpy())\n",
    "    pred_rows.append(_basic_pred_frame(g, yhat, model_name))\n",
    "\n",
    "pred_df = pd.concat(pred_rows, ignore_index=True)\n",
    "_save_pred_df(pred_df, model_name)\n",
    "m = _metrics_from_pred(pred_df, model_name)\n",
    "m = _attach_matched_oos_r2(m, pred_df)\n",
    "all_metrics.append(m)\n",
    "print(m)\n",
    "\n",
    "# 6.4 ridge_pooled\n",
    "model_name = \"ridge_pooled\"\n",
    "num_features = feature_candidates_base.copy()\n",
    "cat_features = [\"Ticker\"]\n",
    "\n",
    "allnan = [c for c in num_features if train_df[c].isna().all()]\n",
    "if allnan:\n",
    "    print(\"Dropping all-NaN train features:\", allnan[:20])\n",
    "    num_features = [c for c in num_features if c not in allnan]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scale\", StandardScaler())\n",
    "        ]), num_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_features)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"prep\", preprocess),\n",
    "    (\"model\", Ridge(alpha=1000.0, random_state=42))\n",
    "])\n",
    "\n",
    "pipe.fit(train_df[num_features + cat_features], train_df[\"target_ret\"])\n",
    "yhat = pipe.predict(df[num_features + cat_features])\n",
    "pred_df = _basic_pred_frame(df, yhat, model_name)\n",
    "_save_pred_df(pred_df, model_name)\n",
    "m = _metrics_from_pred(pred_df, model_name)\n",
    "m = _attach_matched_oos_r2(m, pred_df)\n",
    "all_metrics.append(m)\n",
    "print(m)\n",
    "\n",
    "# 6.5 rf_pooled\n",
    "model_name = \"rf_pooled\"\n",
    "rf_preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", SimpleImputer(strategy=\"median\"), num_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_features)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "rf_pipe = Pipeline([\n",
    "    (\"prep\", rf_preprocess),\n",
    "    (\"model\", RandomForestRegressor(\n",
    "        n_estimators=400,\n",
    "        min_samples_leaf=50,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "rf_pipe.fit(train_df[num_features + cat_features], train_df[\"target_ret\"])\n",
    "yhat = rf_pipe.predict(df[num_features + cat_features])\n",
    "pred_df = _basic_pred_frame(df, yhat, model_name)\n",
    "_save_pred_df(pred_df, model_name)\n",
    "m = _metrics_from_pred(pred_df, model_name)\n",
    "m = _attach_matched_oos_r2(m, pred_df)\n",
    "all_metrics.append(m)\n",
    "print(m)\n",
    "\n",
    "# Save baseline metrics\n",
    "baseline_metrics = pd.concat(all_metrics, ignore_index=True)\n",
    "col_order = [\"model\",\"split\",\"n\",\"MAE\",\"RMSE\",\"Corr\",\"DirectionalAcc\",\"OOS_R2_vs_baseline\"]\n",
    "for c in col_order:\n",
    "    if c not in baseline_metrics.columns:\n",
    "        baseline_metrics[c] = np.nan\n",
    "baseline_metrics = baseline_metrics[col_order].sort_values([\"split\",\"RMSE\"]).reset_index(drop=True)\n",
    "\n",
    "out_path = TAB_DIR / \"baseline_metrics.csv\"\n",
    "baseline_metrics.to_csv(out_path, index=False)\n",
    "print(\"Saved baseline metrics:\", out_path)\n",
    "baseline_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5aa6c42-63b4-4375-87a2-c0d24acc860c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DL ready: make_sequences, build_gru\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "#  Deep Learning Utilities (GRU)\n",
    "# ==========================\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "def make_sequences(df: pd.DataFrame, feature_cols: list[str], seq_len: int):\n",
    "    df = df.sort_values([\"Ticker\",\"Date\"]).copy()\n",
    "    X_list, y_list, meta_rows = [], [], []\n",
    "\n",
    "    for tkr, g in df.groupby(\"Ticker\", sort=False):\n",
    "        g = g.dropna(subset=feature_cols + [\"target_ret\"]).copy()\n",
    "        vals = g[feature_cols].to_numpy(dtype=np.float32)\n",
    "        y = g[\"target_ret\"].to_numpy(dtype=np.float32)\n",
    "\n",
    "        for i in range(seq_len, len(g)):\n",
    "            X_list.append(vals[i-seq_len:i, :])\n",
    "            y_list.append(y[i])\n",
    "            meta_rows.append({\n",
    "                \"Ticker\": tkr,\n",
    "                \"Date\": g.iloc[i][\"Date\"],\n",
    "                \"target_date\": g.iloc[i][\"target_date\"],\n",
    "                \"split\": g.iloc[i][\"split\"],\n",
    "            })\n",
    "\n",
    "    if len(X_list) == 0:\n",
    "        raise ValueError(\"No sequences created. Check NaNs / feature_cols / seq_len.\")\n",
    "\n",
    "    X = np.stack(X_list, axis=0)\n",
    "    y = np.array(y_list, dtype=np.float32)\n",
    "    meta = pd.DataFrame(meta_rows)\n",
    "    return X, y, meta\n",
    "\n",
    "def build_gru(input_shape):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.GRU(64),\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(1e-3), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "print(\"DL ready: make_sequences, build_gru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffaa7a6f-b68e-4455-80d3-0e1ea679a312",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (19536, 30, 18) y: (19536,)\n",
      "Train/Val/Test: (7480, 30, 18) (6048, 30, 18) (6008, 30, 18)\n",
      "Epoch 1/50\n",
      "30/30 [==============================] - 3s 23ms/step - loss: 0.0176 - val_loss: 0.1799 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.0028 - val_loss: 0.0823 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 0.0017 - val_loss: 0.0647 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0516 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.0011 - val_loss: 0.0471 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 9.8979e-04 - val_loss: 0.0402 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 8.6931e-04 - val_loss: 0.0403 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 7.9207e-04 - val_loss: 0.0333 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 7.1653e-04 - val_loss: 0.0342 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 6.6678e-04 - val_loss: 0.0329 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 6.2294e-04 - val_loss: 0.0334 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 5.9885e-04 - val_loss: 0.0321 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 5.5780e-04 - val_loss: 0.0316 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 5.2588e-04 - val_loss: 0.0284 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 5.0129e-04 - val_loss: 0.0279 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 4.8303e-04 - val_loss: 0.0269 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 4.7483e-04 - val_loss: 0.0271 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 4.4839e-04 - val_loss: 0.0303 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 4.2610e-04 - val_loss: 0.0281 - lr: 5.0000e-04\n",
      "Epoch 20/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 4.1333e-04 - val_loss: 0.0272 - lr: 5.0000e-04\n",
      "Epoch 21/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 4.0200e-04 - val_loss: 0.0261 - lr: 2.5000e-04\n",
      "Epoch 22/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 3.9704e-04 - val_loss: 0.0269 - lr: 2.5000e-04\n",
      "Epoch 23/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 3.9297e-04 - val_loss: 0.0264 - lr: 2.5000e-04\n",
      "Epoch 24/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 3.8880e-04 - val_loss: 0.0265 - lr: 1.2500e-04\n",
      "Epoch 25/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 3.8601e-04 - val_loss: 0.0259 - lr: 1.2500e-04\n",
      "Epoch 26/50\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 3.8463e-04 - val_loss: 0.0262 - lr: 1.2500e-04\n",
      "Epoch 27/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 3.8327e-04 - val_loss: 0.0257 - lr: 1.2500e-04\n",
      "Epoch 28/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 3.8164e-04 - val_loss: 0.0256 - lr: 1.2500e-04\n",
      "Epoch 29/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 3.8019e-04 - val_loss: 0.0257 - lr: 1.2500e-04\n",
      "Epoch 30/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 3.7744e-04 - val_loss: 0.0256 - lr: 6.2500e-05\n",
      "Epoch 31/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 3.7574e-04 - val_loss: 0.0257 - lr: 6.2500e-05\n",
      "Epoch 32/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 3.7449e-04 - val_loss: 0.0256 - lr: 3.1250e-05\n",
      "Epoch 33/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 3.7401e-04 - val_loss: 0.0259 - lr: 3.1250e-05\n",
      "Epoch 34/50\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 3.7345e-04 - val_loss: 0.0258 - lr: 3.1250e-05\n",
      "Epoch 35/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 3.7238e-04 - val_loss: 0.0258 - lr: 1.5625e-05\n",
      "Epoch 36/50\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 3.7214e-04 - val_loss: 0.0258 - lr: 1.5625e-05\n",
      "Epoch 37/50\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 3.7173e-04 - val_loss: 0.0258 - lr: 1.0000e-05\n",
      "Saved: stock_project/reports/predictions/gru_base.parquet rows: 19536\n",
      "Saved: stock_project/reports/tables/gru_base_metrics.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>split</th>\n",
       "      <th>n</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Corr</th>\n",
       "      <th>DirectionalAcc</th>\n",
       "      <th>OOS_R2_vs_baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gru_base</td>\n",
       "      <td>train</td>\n",
       "      <td>7480</td>\n",
       "      <td>0.013821</td>\n",
       "      <td>0.019308</td>\n",
       "      <td>0.379172</td>\n",
       "      <td>0.595856</td>\n",
       "      <td>0.000886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gru_base</td>\n",
       "      <td>val</td>\n",
       "      <td>6048</td>\n",
       "      <td>0.138582</td>\n",
       "      <td>0.159862</td>\n",
       "      <td>0.043761</td>\n",
       "      <td>0.520503</td>\n",
       "      <td>-28.857373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gru_base</td>\n",
       "      <td>test</td>\n",
       "      <td>6008</td>\n",
       "      <td>0.104815</td>\n",
       "      <td>0.118774</td>\n",
       "      <td>-0.036799</td>\n",
       "      <td>0.541944</td>\n",
       "      <td>-21.972123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  split     n       MAE      RMSE      Corr  DirectionalAcc  \\\n",
       "0  gru_base  train  7480  0.013821  0.019308  0.379172        0.595856   \n",
       "1  gru_base    val  6048  0.138582  0.159862  0.043761        0.520503   \n",
       "2  gru_base   test  6008  0.104815  0.118774 -0.036799        0.541944   \n",
       "\n",
       "   OOS_R2_vs_baseline  \n",
       "0            0.000886  \n",
       "1          -28.857373  \n",
       "2          -21.972123  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==========================\n",
    "# GRU Baseline (NO GARCH)\n",
    "# ==========================\n",
    "SEQ_LEN = 30\n",
    "FEATURE_COLS = feature_candidates_base  # consistent\n",
    "\n",
    "df = panel.copy()\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df[\"target_date\"] = pd.to_datetime(df[\"target_date\"])\n",
    "df = df.dropna(subset=[\"split\",\"target_ret\"])\n",
    "\n",
    "X, y, meta = make_sequences(df, FEATURE_COLS, seq_len=SEQ_LEN)\n",
    "print(\"X:\", X.shape, \"y:\", y.shape)\n",
    "\n",
    "idx_train = meta[\"split\"].values == \"train\"\n",
    "idx_val   = meta[\"split\"].values == \"val\"\n",
    "idx_test  = meta[\"split\"].values == \"test\"\n",
    "\n",
    "X_train, y_train = X[idx_train], y[idx_train]\n",
    "X_val, y_val     = X[idx_val], y[idx_val]\n",
    "X_test, y_test   = X[idx_test], y[idx_test]\n",
    "print(\"Train/Val/Test:\", X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "# scale using TRAIN only\n",
    "scaler = StandardScaler()\n",
    "F = X.shape[-1]\n",
    "scaler.fit(X_train.reshape(-1, F))\n",
    "\n",
    "def scale_X(X_in):\n",
    "    X2 = X_in.reshape(-1, F)\n",
    "    X2s = scaler.transform(X2)\n",
    "    return X2s.reshape(X_in.shape)\n",
    "\n",
    "X_train_s = scale_X(X_train)\n",
    "X_val_s   = scale_X(X_val)\n",
    "X_test_s  = scale_X(X_test)\n",
    "\n",
    "model = build_gru((SEQ_LEN, F))\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-5)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_s, y_train,\n",
    "    validation_data=(X_val_s, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=256,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "yhat_train = model.predict(X_train_s, verbose=0).reshape(-1)\n",
    "yhat_val   = model.predict(X_val_s, verbose=0).reshape(-1)\n",
    "yhat_test  = model.predict(X_test_s, verbose=0).reshape(-1)\n",
    "\n",
    "pred_train = meta.loc[idx_train, [\"Date\",\"target_date\",\"Ticker\",\"split\"]].copy()\n",
    "pred_val   = meta.loc[idx_val,   [\"Date\",\"target_date\",\"Ticker\",\"split\"]].copy()\n",
    "pred_test  = meta.loc[idx_test,  [\"Date\",\"target_date\",\"Ticker\",\"split\"]].copy()\n",
    "\n",
    "for p, yt, yp in [(pred_train,y_train,yhat_train),(pred_val,y_val,yhat_val),(pred_test,y_test,yhat_test)]:\n",
    "    p[\"model\"] = \"gru_base\"\n",
    "    p[\"y_true\"] = yt\n",
    "    p[\"y_pred\"] = yp\n",
    "pred_df = pd.concat([pred_train,pred_val,pred_test], ignore_index=True)\n",
    "pred_df[\"residual\"] = pred_df[\"y_true\"] - pred_df[\"y_pred\"]\n",
    "\n",
    "pred_path = PRED_DIR / \"gru_base.parquet\"\n",
    "pred_df.to_parquet(pred_path, index=False)\n",
    "print(\"Saved:\", pred_path, \"rows:\", len(pred_df))\n",
    "\n",
    "# metrics\n",
    "rows = []\n",
    "for split in [\"train\",\"val\",\"test\"]:\n",
    "    d = pred_df[pred_df[\"split\"]==split]\n",
    "    m = regression_metrics(d[\"y_true\"].to_numpy(), d[\"y_pred\"].to_numpy())\n",
    "    rows.append({\"model\":\"gru_base\",\"split\":split,\"n\":int(len(d)),**m})\n",
    "metrics_df = pd.DataFrame(rows)\n",
    "\n",
    "oos = oos_r2_vs_baseline_matched(pred_df, PRED_DIR / \"baseline_ticker_mean.parquet\")\n",
    "metrics_df[\"OOS_R2_vs_baseline\"] = metrics_df[\"split\"].map(oos)\n",
    "\n",
    "out_path = TAB_DIR / \"gru_base_metrics.csv\"\n",
    "metrics_df.to_csv(out_path, index=False)\n",
    "print(\"Saved:\", out_path)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7118bab0-c645-4e99-a498-c48ff888f94a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
