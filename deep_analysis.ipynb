{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05669717-89d7-47db-96af-e48962152e12",
   "metadata": {},
   "source": [
    "# Setup (Paths, Seeds, Reproducibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd3cbb58-73ba-4f9e-aba2-fbaf5f53a194",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_DIR: /notebooks/stock_project\n",
      "PROC_DIR: /notebooks/stock_project/data/processed\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Global Setup (RUN ONCE)\n",
    "# ==========================\n",
    "import os, json, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "PROJECT_DIR = Path(\"stock_project\")\n",
    "DATA_DIR = PROJECT_DIR / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROC_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "REPORT_DIR = PROJECT_DIR / \"reports\"\n",
    "PRED_DIR = REPORT_DIR / \"predictions\"\n",
    "TAB_DIR  = REPORT_DIR / \"tables\"\n",
    "FIG_DIR  = REPORT_DIR / \"figures\"\n",
    "MODEL_DIR = PROJECT_DIR / \"models\"\n",
    "\n",
    "for d in [RAW_DIR, PROC_DIR, PRED_DIR, TAB_DIR, FIG_DIR, MODEL_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR.resolve())\n",
    "print(\"PROC_DIR:\", PROC_DIR.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeca9e6-8940-4a14-bb20-a5dd2a35b8c8",
   "metadata": {},
   "source": [
    "# Load Panel and Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55826af0-f1c8-4af6-80bc-427d68fbac46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "panel shape: (19776, 23)\n",
      "tickers: 8\n",
      "split\n",
      "train    7720\n",
      "val      6048\n",
      "test     6008\n",
      "Name: count, dtype: int64\n",
      "        Date target_date Ticker  target_ret  split\n",
      "0 2016-03-02  2016-03-03   AAPL    0.007417  train\n",
      "1 2016-03-03  2016-03-04   AAPL    0.014767  train\n",
      "2 2016-03-04  2016-03-07   AAPL   -0.011129  train\n",
      "3 2016-03-07  2016-03-08   AAPL   -0.008280  train\n",
      "4 2016-03-08  2016-03-09   AAPL    0.000890  train\n"
     ]
    }
   ],
   "source": [
    "panel_path = PROC_DIR / \"panel.parquet\"\n",
    "assert panel_path.exists(), f\"Missing {panel_path}. Build the panel first.\"\n",
    "\n",
    "panel = pd.read_parquet(panel_path).copy()\n",
    "panel[\"Date\"] = pd.to_datetime(panel[\"Date\"])\n",
    "panel[\"target_date\"] = pd.to_datetime(panel[\"target_date\"])\n",
    "\n",
    "print(\"panel shape:\", panel.shape)\n",
    "print(\"tickers:\", panel[\"Ticker\"].nunique())\n",
    "print(panel[\"split\"].value_counts(dropna=False))\n",
    "print(panel[[\"Date\",\"target_date\",\"Ticker\",\"target_ret\",\"split\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2da79758-a66f-4a56-9b2f-fff2e5fcda80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilities loaded: regression_metrics, directional_accuracy, oos_r2_vs_baseline_matched\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 3. Metrics & Utilities (Publication-grade)\n",
    "# =========================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def directional_accuracy(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 0.0) -> float:\n",
    "    \"\"\"\n",
    "    Direction accuracy using >0 definition (NOT np.sign equality).\n",
    "    eps allows ignoring tiny moves (e.g., 5 bps => eps=0.0005)\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true).reshape(-1)\n",
    "    y_pred = np.asarray(y_pred).reshape(-1)\n",
    "    return float(np.mean((y_true > eps) == (y_pred > eps)))\n",
    "\n",
    "def regression_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> dict:\n",
    "    y_true = np.asarray(y_true).reshape(-1)\n",
    "    y_pred = np.asarray(y_pred).reshape(-1)\n",
    "\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "    # Pearson correlation (guard for constant arrays)\n",
    "    corr = np.nan\n",
    "    if np.std(y_true) > 0 and np.std(y_pred) > 0:\n",
    "        corr = float(np.corrcoef(y_true, y_pred)[0, 1])\n",
    "\n",
    "    dir_acc = directional_accuracy(y_true, y_pred, eps=0.0)\n",
    "\n",
    "    return {\n",
    "        \"MAE\": float(mae),\n",
    "        \"RMSE\": float(rmse),\n",
    "        \"Corr\": corr,\n",
    "        \"DirectionalAcc\": float(dir_acc),\n",
    "    }\n",
    "\n",
    "def oos_r2_vs_baseline_matched(pred_df: pd.DataFrame, baseline_pred_path: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Computes OOS R^2 vs baseline on the EXACT SAME ROWS by merging baseline predictions.\n",
    "    pred_df must have: Ticker, target_date, split, y_true, y_pred\n",
    "    baseline file should have: Ticker, target_date, y_pred\n",
    "    \"\"\"\n",
    "    if not baseline_pred_path.exists():\n",
    "        return {s: np.nan for s in [\"train\", \"val\", \"test\"]}\n",
    "\n",
    "    base = pd.read_parquet(baseline_pred_path).copy()\n",
    "    base[\"target_date\"] = pd.to_datetime(base[\"target_date\"])\n",
    "    base = base.rename(columns={\"y_pred\": \"y_pred_baseline\"})[[\"Ticker\", \"target_date\", \"y_pred_baseline\"]]\n",
    "\n",
    "    merged = pred_df.merge(base, on=[\"Ticker\", \"target_date\"], how=\"left\", validate=\"many_to_one\")\n",
    "    out = {}\n",
    "\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        d = merged[merged[\"split\"] == split].dropna(subset=[\"y_pred_baseline\"]).copy()\n",
    "        if len(d) == 0:\n",
    "            out[split] = np.nan\n",
    "            continue\n",
    "        mse_model = float(np.mean((d[\"y_true\"] - d[\"y_pred\"]) ** 2))\n",
    "        mse_base  = float(np.mean((d[\"y_true\"] - d[\"y_pred_baseline\"]) ** 2))\n",
    "        out[split] = np.nan if mse_base <= 0 else float(1.0 - mse_model / mse_base)\n",
    "\n",
    "    return out\n",
    "\n",
    "print(\"Utilities loaded: regression_metrics, directional_accuracy, oos_r2_vs_baseline_matched\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8b060a-1a6c-45b5-bcae-bf0bec36d83a",
   "metadata": {},
   "source": [
    "#  Feature Set Definition (Consistent across models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c186702c-01e3-45a3-8ecd-1fae6cb08552",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num features (base): 18\n",
      "['DFF_diff_lag1', 'DFF_lag1', 'DGS10_diff_lag1', 'DGS10_lag1', 'SP500_lag1', 'mkt_ret_lag1', 'ret', 'ret_lag1', 'ret_lag10', 'ret_lag2', 'ret_lag3', 'ret_lag5', 'ret_vol10', 'ret_vol20', 'ret_vol5'] ...\n"
     ]
    }
   ],
   "source": [
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "NON_FEATURES = {\"Date\", \"target_date\", \"split\", \"Ticker\", \"target_ret\", \"has_garch\"}\n",
    "\n",
    "def numeric_feature_candidates(df: pd.DataFrame) -> list[str]:\n",
    "    feats = []\n",
    "    for c in df.columns:\n",
    "        if c in NON_FEATURES:\n",
    "            continue\n",
    "        if is_numeric_dtype(df[c]):\n",
    "            feats.append(c)\n",
    "    return sorted(feats)\n",
    "\n",
    "feature_candidates_base = numeric_feature_candidates(panel)\n",
    "print(\"Num features (base):\", len(feature_candidates_base))\n",
    "print(feature_candidates_base[:15], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b93e0ae-29c1-41de-ae53-0e12ab07c938",
   "metadata": {},
   "source": [
    "#  Baseline Mean Models (Zero, Ticker-Mean, AR1, Ridge, RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cad93a68-9a80-4513-835e-8816f7d2bad7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes: {'train': 7720, 'val': 6048, 'test': 6008}\n",
      "Saved predictions: stock_project/reports/predictions/baseline_zero.parquet | rows=19,776\n",
      "           model  split     n       MAE      RMSE  Corr  DirectionalAcc\n",
      "0  baseline_zero  train  7720  0.012403  0.019210   NaN        0.461140\n",
      "1  baseline_zero    val  6048  0.019788  0.029259   NaN        0.477844\n",
      "2  baseline_zero   test  6008  0.016531  0.024833   NaN        0.457390\n",
      "Saved predictions: stock_project/reports/predictions/baseline_ticker_mean.parquet | rows=19,776\n",
      "                  model  split     n       MAE      RMSE      Corr  \\\n",
      "0  baseline_ticker_mean  train  7720  0.012353  0.019176  0.025203   \n",
      "1  baseline_ticker_mean    val  6048  0.019762  0.029256  0.007244   \n",
      "2  baseline_ticker_mean   test  6008  0.016471  0.024781  0.015846   \n",
      "\n",
      "   DirectionalAcc  OOS_R2_vs_baseline  \n",
      "0        0.538860                 0.0  \n",
      "1        0.522156                 0.0  \n",
      "2        0.542610                 0.0  \n",
      "AR1 regressor column: ret_lag1\n",
      "Saved predictions: stock_project/reports/predictions/ar1_per_ticker.parquet | rows=19,776\n",
      "            model  split     n       MAE      RMSE      Corr  DirectionalAcc  \\\n",
      "0  ar1_per_ticker  train  7720  0.012353  0.019163  0.044651        0.536269   \n",
      "1  ar1_per_ticker    val  6048  0.019799  0.029304 -0.018409        0.514054   \n",
      "2  ar1_per_ticker   test  6008  0.016473  0.024786  0.020092        0.542610   \n",
      "\n",
      "   OOS_R2_vs_baseline  \n",
      "0            0.001359  \n",
      "1           -0.003284  \n",
      "2           -0.000373  \n",
      "Ridge num features: 18\n",
      "Saved predictions: stock_project/reports/predictions/ridge_pooled.parquet | rows=19,776\n",
      "          model  split     n       MAE      RMSE      Corr  DirectionalAcc  \\\n",
      "0  ridge_pooled  train  7720  0.012329  0.019092  0.098605        0.540026   \n",
      "1  ridge_pooled    val  6048  0.019867  0.029437 -0.006374        0.527943   \n",
      "2  ridge_pooled   test  6008  0.017224  0.025323  0.039794        0.460220   \n",
      "\n",
      "   OOS_R2_vs_baseline  \n",
      "0            0.008793  \n",
      "1           -0.012363  \n",
      "2           -0.044188  \n",
      "Saved predictions: stock_project/reports/predictions/rf_pooled.parquet | rows=19,776\n",
      "       model  split     n       MAE      RMSE      Corr  DirectionalAcc  \\\n",
      "0  rf_pooled  train  7720  0.011018  0.017436  0.580693        0.704793   \n",
      "1  rf_pooled    val  6048  0.020011  0.029469  0.013791        0.503638   \n",
      "2  rf_pooled   test  6008  0.019257  0.027006  0.045119        0.462051   \n",
      "\n",
      "   OOS_R2_vs_baseline  \n",
      "0            0.173290  \n",
      "1           -0.014568  \n",
      "2           -0.187616  \n",
      "Saved baseline metrics: stock_project/reports/tables/baseline_metrics.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>split</th>\n",
       "      <th>n</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Corr</th>\n",
       "      <th>DirectionalAcc</th>\n",
       "      <th>OOS_R2_vs_baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline_ticker_mean</td>\n",
       "      <td>test</td>\n",
       "      <td>6008</td>\n",
       "      <td>0.016471</td>\n",
       "      <td>0.024781</td>\n",
       "      <td>0.015846</td>\n",
       "      <td>0.542610</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ar1_per_ticker</td>\n",
       "      <td>test</td>\n",
       "      <td>6008</td>\n",
       "      <td>0.016473</td>\n",
       "      <td>0.024786</td>\n",
       "      <td>0.020092</td>\n",
       "      <td>0.542610</td>\n",
       "      <td>-0.000373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>baseline_zero</td>\n",
       "      <td>test</td>\n",
       "      <td>6008</td>\n",
       "      <td>0.016531</td>\n",
       "      <td>0.024833</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.457390</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ridge_pooled</td>\n",
       "      <td>test</td>\n",
       "      <td>6008</td>\n",
       "      <td>0.017224</td>\n",
       "      <td>0.025323</td>\n",
       "      <td>0.039794</td>\n",
       "      <td>0.460220</td>\n",
       "      <td>-0.044188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rf_pooled</td>\n",
       "      <td>test</td>\n",
       "      <td>6008</td>\n",
       "      <td>0.019257</td>\n",
       "      <td>0.027006</td>\n",
       "      <td>0.045119</td>\n",
       "      <td>0.462051</td>\n",
       "      <td>-0.187616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rf_pooled</td>\n",
       "      <td>train</td>\n",
       "      <td>7720</td>\n",
       "      <td>0.011018</td>\n",
       "      <td>0.017436</td>\n",
       "      <td>0.580693</td>\n",
       "      <td>0.704793</td>\n",
       "      <td>0.173290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ridge_pooled</td>\n",
       "      <td>train</td>\n",
       "      <td>7720</td>\n",
       "      <td>0.012329</td>\n",
       "      <td>0.019092</td>\n",
       "      <td>0.098605</td>\n",
       "      <td>0.540026</td>\n",
       "      <td>0.008793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ar1_per_ticker</td>\n",
       "      <td>train</td>\n",
       "      <td>7720</td>\n",
       "      <td>0.012353</td>\n",
       "      <td>0.019163</td>\n",
       "      <td>0.044651</td>\n",
       "      <td>0.536269</td>\n",
       "      <td>0.001359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>baseline_ticker_mean</td>\n",
       "      <td>train</td>\n",
       "      <td>7720</td>\n",
       "      <td>0.012353</td>\n",
       "      <td>0.019176</td>\n",
       "      <td>0.025203</td>\n",
       "      <td>0.538860</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>baseline_zero</td>\n",
       "      <td>train</td>\n",
       "      <td>7720</td>\n",
       "      <td>0.012403</td>\n",
       "      <td>0.019210</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.461140</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>baseline_ticker_mean</td>\n",
       "      <td>val</td>\n",
       "      <td>6048</td>\n",
       "      <td>0.019762</td>\n",
       "      <td>0.029256</td>\n",
       "      <td>0.007244</td>\n",
       "      <td>0.522156</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>baseline_zero</td>\n",
       "      <td>val</td>\n",
       "      <td>6048</td>\n",
       "      <td>0.019788</td>\n",
       "      <td>0.029259</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.477844</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ar1_per_ticker</td>\n",
       "      <td>val</td>\n",
       "      <td>6048</td>\n",
       "      <td>0.019799</td>\n",
       "      <td>0.029304</td>\n",
       "      <td>-0.018409</td>\n",
       "      <td>0.514054</td>\n",
       "      <td>-0.003284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ridge_pooled</td>\n",
       "      <td>val</td>\n",
       "      <td>6048</td>\n",
       "      <td>0.019867</td>\n",
       "      <td>0.029437</td>\n",
       "      <td>-0.006374</td>\n",
       "      <td>0.527943</td>\n",
       "      <td>-0.012363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rf_pooled</td>\n",
       "      <td>val</td>\n",
       "      <td>6048</td>\n",
       "      <td>0.020011</td>\n",
       "      <td>0.029469</td>\n",
       "      <td>0.013791</td>\n",
       "      <td>0.503638</td>\n",
       "      <td>-0.014568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   model  split     n       MAE      RMSE      Corr  \\\n",
       "0   baseline_ticker_mean   test  6008  0.016471  0.024781  0.015846   \n",
       "1         ar1_per_ticker   test  6008  0.016473  0.024786  0.020092   \n",
       "2          baseline_zero   test  6008  0.016531  0.024833       NaN   \n",
       "3           ridge_pooled   test  6008  0.017224  0.025323  0.039794   \n",
       "4              rf_pooled   test  6008  0.019257  0.027006  0.045119   \n",
       "5              rf_pooled  train  7720  0.011018  0.017436  0.580693   \n",
       "6           ridge_pooled  train  7720  0.012329  0.019092  0.098605   \n",
       "7         ar1_per_ticker  train  7720  0.012353  0.019163  0.044651   \n",
       "8   baseline_ticker_mean  train  7720  0.012353  0.019176  0.025203   \n",
       "9          baseline_zero  train  7720  0.012403  0.019210       NaN   \n",
       "10  baseline_ticker_mean    val  6048  0.019762  0.029256  0.007244   \n",
       "11         baseline_zero    val  6048  0.019788  0.029259       NaN   \n",
       "12        ar1_per_ticker    val  6048  0.019799  0.029304 -0.018409   \n",
       "13          ridge_pooled    val  6048  0.019867  0.029437 -0.006374   \n",
       "14             rf_pooled    val  6048  0.020011  0.029469  0.013791   \n",
       "\n",
       "    DirectionalAcc  OOS_R2_vs_baseline  \n",
       "0         0.542610            0.000000  \n",
       "1         0.542610           -0.000373  \n",
       "2         0.457390                 NaN  \n",
       "3         0.460220           -0.044188  \n",
       "4         0.462051           -0.187616  \n",
       "5         0.704793            0.173290  \n",
       "6         0.540026            0.008793  \n",
       "7         0.536269            0.001359  \n",
       "8         0.538860            0.000000  \n",
       "9         0.461140                 NaN  \n",
       "10        0.522156            0.000000  \n",
       "11        0.477844                 NaN  \n",
       "12        0.514054           -0.003284  \n",
       "13        0.527943           -0.012363  \n",
       "14        0.503638           -0.014568  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP 5 â€” Baseline Mean Models (FULL COPY/PASTE, RUN TOP->BOTTOM)\n",
    "# Models:\n",
    "#   1) baseline_zero\n",
    "#   2) baseline_ticker_mean  (this becomes the reference baseline)\n",
    "#   3) ar1_per_ticker\n",
    "#   4) ridge_pooled (numeric + ticker one-hot)\n",
    "#   5) rf_pooled    (numeric + ticker one-hot)\n",
    "# Saves:\n",
    "#   - reports/predictions/<model>.parquet\n",
    "#   - reports/tables/baseline_metrics.csv\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# ----------------------------\n",
    "# Preconditions (must exist from earlier steps)\n",
    "# - panel (loaded from panel.parquet)\n",
    "# - feature_candidates_base (from numeric_feature_candidates(panel))\n",
    "# - PRED_DIR, TAB_DIR\n",
    "# - regression_metrics(...)\n",
    "# - oos_r2_vs_baseline_matched(...)\n",
    "# ----------------------------\n",
    "\n",
    "assert \"panel\" in globals(), \"Run Step 2 (load panel) first.\"\n",
    "assert \"feature_candidates_base\" in globals(), \"Run Step 4 (feature candidates) first.\"\n",
    "assert \"PRED_DIR\" in globals() and \"TAB_DIR\" in globals(), \"Run Step 1 (paths) first.\"\n",
    "assert \"regression_metrics\" in globals(), \"Run Step 3 (metrics utilities) first.\"\n",
    "\n",
    "df = panel.copy()\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df[\"target_date\"] = pd.to_datetime(df[\"target_date\"])\n",
    "\n",
    "# Ensure target exists\n",
    "assert \"target_ret\" in df.columns, \"panel must contain target_ret.\"\n",
    "\n",
    "# A strict time-respecting split is assumed already encoded in df['split']\n",
    "assert \"split\" in df.columns, \"panel must contain split column with train/val/test.\"\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "\n",
    "def _save_pred_df(pred_df: pd.DataFrame, model_name: str) -> Path:\n",
    "    pred_df = pred_df.copy()\n",
    "    pred_df[\"target_date\"] = pd.to_datetime(pred_df[\"target_date\"])\n",
    "    path = PRED_DIR / f\"{model_name}.parquet\"\n",
    "    pred_df.to_parquet(path, index=False)\n",
    "    print(f\"Saved predictions: {path} | rows={len(pred_df):,}\")\n",
    "    return path\n",
    "\n",
    "def _metrics_from_pred(pred_df: pd.DataFrame, model_name: str) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        d = pred_df[pred_df[\"split\"] == split]\n",
    "        if len(d) == 0:\n",
    "            continue\n",
    "        m = regression_metrics(d[\"y_true\"].to_numpy(), d[\"y_pred\"].to_numpy())\n",
    "        rows.append({\"model\": model_name, \"split\": split, \"n\": int(len(d)), **m})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def _attach_matched_oos_r2(metrics_df: pd.DataFrame, pred_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    baseline_path = PRED_DIR / \"baseline_ticker_mean.parquet\"\n",
    "    oos = oos_r2_vs_baseline_matched(pred_df, baseline_path)\n",
    "    metrics_df = metrics_df.copy()\n",
    "    metrics_df[\"OOS_R2_vs_baseline\"] = metrics_df[\"split\"].map(oos)\n",
    "    return metrics_df\n",
    "\n",
    "def _basic_pred_frame(df_sub: pd.DataFrame, y_pred: np.ndarray, model_name: str) -> pd.DataFrame:\n",
    "    out = df_sub[[\"Date\", \"target_date\", \"Ticker\", \"split\"]].copy()\n",
    "    out[\"model\"] = model_name\n",
    "    out[\"y_true\"] = df_sub[\"target_ret\"].to_numpy()\n",
    "    out[\"y_pred\"] = np.asarray(y_pred).reshape(-1)\n",
    "    out[\"residual\"] = out[\"y_true\"] - out[\"y_pred\"]\n",
    "    return out\n",
    "\n",
    "def _find_ar1_feature(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    We prefer a 1-day lag of returns as AR(1) regressor.\n",
    "    This tries to find an existing column. If none exists, it builds one\n",
    "    from target_ret by shifting within ticker (safe: uses past info).\n",
    "    \"\"\"\n",
    "    candidates = [\n",
    "        \"ret_lag1\", \"ret_lag_1\", \"ret_l1\", \"lag1_ret\", \"return_lag1\", \"logret_lag1\"\n",
    "    ]\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "\n",
    "    # Try fuzzy find\n",
    "    fuzzy = [c for c in df.columns if (\"lag\" in c.lower() and \"ret\" in c.lower() and \"1\" in c)]\n",
    "    if len(fuzzy) > 0:\n",
    "        return fuzzy[0]\n",
    "\n",
    "    # Build from target_ret: ret at t is target_ret shifted back one day within ticker\n",
    "    # target_ret at t corresponds to ret_{t+1}; shifting by 1 gives a proxy for ret_t.\n",
    "    df[\"ar1_x\"] = df.groupby(\"Ticker\", sort=False)[\"target_ret\"].shift(1)\n",
    "    return \"ar1_x\"\n",
    "\n",
    "# ----------------------------\n",
    "# Create base split frames\n",
    "# ----------------------------\n",
    "train_df = df[df[\"split\"] == \"train\"].copy()\n",
    "val_df   = df[df[\"split\"] == \"val\"].copy()\n",
    "test_df  = df[df[\"split\"] == \"test\"].copy()\n",
    "\n",
    "print(\"Split sizes:\", {k: int(v) for k, v in df[\"split\"].value_counts().to_dict().items()})\n",
    "\n",
    "all_metrics = []\n",
    "\n",
    "# ============================================================\n",
    "# 5.1 Baseline: Zero predictor\n",
    "# ============================================================\n",
    "model_name = \"baseline_zero\"\n",
    "y_pred = np.zeros(len(df), dtype=float)\n",
    "pred_df = _basic_pred_frame(df, y_pred, model_name)\n",
    "_save_pred_df(pred_df, model_name)\n",
    "\n",
    "m = _metrics_from_pred(pred_df, model_name)\n",
    "# OOS_R2_vs_baseline will be NaN here because baseline file might not exist yet; that's fine.\n",
    "all_metrics.append(m)\n",
    "\n",
    "print(m)\n",
    "\n",
    "# ============================================================\n",
    "# 5.2 Baseline: Per-ticker mean (TRAIN only)  <-- reference baseline\n",
    "# ============================================================\n",
    "model_name = \"baseline_ticker_mean\"\n",
    "ticker_mean = train_df.groupby(\"Ticker\")[\"target_ret\"].mean()\n",
    "\n",
    "# If a ticker is missing in train (rare), fall back to global train mean\n",
    "global_mean = float(train_df[\"target_ret\"].mean())\n",
    "y_pred = df[\"Ticker\"].map(ticker_mean).fillna(global_mean).to_numpy()\n",
    "\n",
    "pred_df = _basic_pred_frame(df, y_pred, model_name)\n",
    "baseline_path = _save_pred_df(pred_df, model_name)\n",
    "\n",
    "m = _metrics_from_pred(pred_df, model_name)\n",
    "m = _attach_matched_oos_r2(m, pred_df)  # now baseline exists; this will be ~0 in expectation vs itself\n",
    "all_metrics.append(m)\n",
    "\n",
    "print(m)\n",
    "\n",
    "# ============================================================\n",
    "# 5.3 AR(1) per ticker (fit on TRAIN only within each ticker)\n",
    "# ============================================================\n",
    "model_name = \"ar1_per_ticker\"\n",
    "\n",
    "ar1_col = _find_ar1_feature(df)\n",
    "print(\"AR1 regressor column:\", ar1_col)\n",
    "\n",
    "pred_rows = []\n",
    "for tkr, g in df.sort_values([\"Ticker\", \"Date\"]).groupby(\"Ticker\", sort=False):\n",
    "    g = g.copy()\n",
    "    # Use only rows where x and y exist\n",
    "    g = g.dropna(subset=[ar1_col, \"target_ret\"])\n",
    "    if len(g) < 50:\n",
    "        # too few points, fallback to train mean\n",
    "        yhat = np.full(len(g), float(ticker_mean.get(tkr, global_mean)))\n",
    "        pred_rows.append(_basic_pred_frame(g, yhat, model_name))\n",
    "        continue\n",
    "\n",
    "    g_tr = g[g[\"split\"] == \"train\"]\n",
    "    if len(g_tr) < 30:\n",
    "        yhat = np.full(len(g), float(ticker_mean.get(tkr, global_mean)))\n",
    "        pred_rows.append(_basic_pred_frame(g, yhat, model_name))\n",
    "        continue\n",
    "\n",
    "    X_tr = g_tr[[ar1_col]].to_numpy()\n",
    "    y_tr = g_tr[\"target_ret\"].to_numpy()\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_tr, y_tr)\n",
    "\n",
    "    X_all = g[[ar1_col]].to_numpy()\n",
    "    yhat = lr.predict(X_all)\n",
    "\n",
    "    pred_rows.append(_basic_pred_frame(g, yhat, model_name))\n",
    "\n",
    "pred_df = pd.concat(pred_rows, ignore_index=True)\n",
    "_save_pred_df(pred_df, model_name)\n",
    "\n",
    "m = _metrics_from_pred(pred_df, model_name)\n",
    "m = _attach_matched_oos_r2(m, pred_df)\n",
    "all_metrics.append(m)\n",
    "\n",
    "print(m)\n",
    "\n",
    "# ============================================================\n",
    "# 5.4 Ridge (pooled across tickers, numeric + ticker one-hot)\n",
    "# ============================================================\n",
    "model_name = \"ridge_pooled\"\n",
    "\n",
    "# Choose a stable numeric feature set\n",
    "num_features = feature_candidates_base.copy()\n",
    "cat_features = [\"Ticker\"]\n",
    "\n",
    "# Drop any features that are all-NaN in train (safety)\n",
    "allnan = [c for c in num_features if train_df[c].isna().all()]\n",
    "if len(allnan) > 0:\n",
    "    print(\"Dropping all-NaN train features:\", allnan[:20], \"...\" if len(allnan) > 20 else \"\")\n",
    "    num_features = [c for c in num_features if c not in allnan]\n",
    "\n",
    "print(\"Ridge num features:\", len(num_features))\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scale\", StandardScaler())\n",
    "        ]), num_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_features)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "ridge = Ridge(alpha=1000.0, random_state=42)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"prep\", preprocess),\n",
    "    (\"model\", ridge)\n",
    "])\n",
    "\n",
    "# Fit on TRAIN only\n",
    "train_fit = train_df.dropna(subset=[\"target_ret\"]).copy()\n",
    "pipe.fit(train_fit[num_features + cat_features], train_fit[\"target_ret\"])\n",
    "\n",
    "# Predict on all rows where features exist (imputer handles NaNs in numerics)\n",
    "yhat = pipe.predict(df[num_features + cat_features])\n",
    "pred_df = _basic_pred_frame(df, yhat, model_name)\n",
    "_save_pred_df(pred_df, model_name)\n",
    "\n",
    "m = _metrics_from_pred(pred_df, model_name)\n",
    "m = _attach_matched_oos_r2(m, pred_df)\n",
    "all_metrics.append(m)\n",
    "\n",
    "print(m)\n",
    "\n",
    "# ============================================================\n",
    "# 5.5 RandomForest (pooled, numeric + ticker one-hot)\n",
    "# Notes:\n",
    "# - No scaling needed\n",
    "# - Still uses imputation for numerics\n",
    "# ============================================================\n",
    "model_name = \"rf_pooled\"\n",
    "\n",
    "rf_preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", SimpleImputer(strategy=\"median\"), num_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_features)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=400,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=50,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_pipe = Pipeline([\n",
    "    (\"prep\", rf_preprocess),\n",
    "    (\"model\", rf)\n",
    "])\n",
    "\n",
    "train_fit = train_df.dropna(subset=[\"target_ret\"]).copy()\n",
    "rf_pipe.fit(train_fit[num_features + cat_features], train_fit[\"target_ret\"])\n",
    "\n",
    "yhat = rf_pipe.predict(df[num_features + cat_features])\n",
    "pred_df = _basic_pred_frame(df, yhat, model_name)\n",
    "_save_pred_df(pred_df, model_name)\n",
    "\n",
    "m = _metrics_from_pred(pred_df, model_name)\n",
    "m = _attach_matched_oos_r2(m, pred_df)\n",
    "all_metrics.append(m)\n",
    "\n",
    "print(m)\n",
    "\n",
    "# ============================================================\n",
    "# 5.6 Consolidate and save baseline metrics table\n",
    "# ============================================================\n",
    "baseline_metrics = pd.concat(all_metrics, ignore_index=True)\n",
    "\n",
    "# Order columns nicely\n",
    "col_order = [\"model\", \"split\", \"n\", \"MAE\", \"RMSE\", \"Corr\", \"DirectionalAcc\", \"OOS_R2_vs_baseline\"]\n",
    "for c in col_order:\n",
    "    if c not in baseline_metrics.columns:\n",
    "        baseline_metrics[c] = np.nan\n",
    "baseline_metrics = baseline_metrics[col_order]\n",
    "\n",
    "baseline_metrics = baseline_metrics.sort_values([\"split\", \"RMSE\", \"MAE\"], ascending=[True, True, True]).reset_index(drop=True)\n",
    "\n",
    "out_path = TAB_DIR / \"baseline_metrics.csv\"\n",
    "baseline_metrics.to_csv(out_path, index=False)\n",
    "print(\"Saved baseline metrics:\", out_path)\n",
    "\n",
    "baseline_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "368f1ecd-eded-4035-b26e-f26ae6fbbf0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-28 15:41:30.333497: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-02-28 15:41:30.334244: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-02-28 15:41:30.408146: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-02-28 15:41:30.581574: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-02-28 15:41:32.386745: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DL utilities loaded: make_sequences, build_gru\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 6. Deep Learning: Shared Setup & Utilities\n",
    "# =========================================\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "def make_sequences(df: pd.DataFrame, feature_cols: list[str], seq_len: int):\n",
    "    \"\"\"\n",
    "    Build sequences per ticker in time order.\n",
    "    Returns X (N, seq_len, F), y (N,), meta dataframe aligning sequences to target_date.\n",
    "    \"\"\"\n",
    "    df = df.sort_values([\"Ticker\", \"Date\"]).copy()\n",
    "    X_list, y_list, meta_rows = [], [], []\n",
    "\n",
    "    for tkr, g in df.groupby(\"Ticker\", sort=False):\n",
    "        g = g.dropna(subset=feature_cols + [\"target_ret\"]).copy()\n",
    "        vals = g[feature_cols].to_numpy(dtype=np.float32)\n",
    "        y = g[\"target_ret\"].to_numpy(dtype=np.float32)\n",
    "\n",
    "        # sequence end index i predicts y[i] using past seq_len rows ending at i-1\n",
    "        for i in range(seq_len, len(g)):\n",
    "            X_list.append(vals[i-seq_len:i, :])\n",
    "            y_list.append(y[i])\n",
    "            meta_rows.append({\n",
    "                \"Ticker\": tkr,\n",
    "                \"Date\": g.iloc[i][\"Date\"],\n",
    "                \"target_date\": g.iloc[i][\"target_date\"],\n",
    "                \"split\": g.iloc[i][\"split\"],\n",
    "            })\n",
    "\n",
    "    if len(X_list) == 0:\n",
    "        raise ValueError(\"No sequences created. Check NaNs, feature_cols, or seq_len.\")\n",
    "\n",
    "    X = np.stack(X_list, axis=0)\n",
    "    y = np.array(y_list, dtype=np.float32)\n",
    "    meta = pd.DataFrame(meta_rows)\n",
    "    return X, y, meta\n",
    "\n",
    "def build_gru(input_shape):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.GRU(64),\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(1e-3), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "print(\"DL utilities loaded: make_sequences, build_gru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e34ee37-1cc5-4250-b010-4a05dfaa0780",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (19536, 30, 18) | y shape: (19536,) | meta: (19536, 4)\n",
      "Train/Val/Test: (7480, 30, 18) (6048, 30, 18) (6008, 30, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-28 15:43:53.735088: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2026-02-28 15:43:54.082880: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2026-02-28 15:43:54.083178: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2026-02-28 15:43:54.084250: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2026-02-28 15:43:54.084463: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2026-02-28 15:43:54.084645: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2026-02-28 15:43:54.338482: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2026-02-28 15:43:54.338843: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2026-02-28 15:43:54.339052: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2026-02-28 15:43:54.339209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15657 MB memory:  -> device: 0, name: Quadro P5000, pci bus id: 0000:00:05.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-28 15:43:57.331336: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "2026-02-28 15:43:58.036730: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fc898868910 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2026-02-28 15:43:58.036790: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P5000, Compute Capability 6.1\n",
      "2026-02-28 15:43:58.063897: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1772293438.215873     442 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 4s 23ms/step - loss: 0.0176 - val_loss: 0.1799 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.0028 - val_loss: 0.0823 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.0017 - val_loss: 0.0647 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.0014 - val_loss: 0.0516 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.0011 - val_loss: 0.0471 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 9.8979e-04 - val_loss: 0.0402 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 8.6931e-04 - val_loss: 0.0403 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 7.9207e-04 - val_loss: 0.0333 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 7.1653e-04 - val_loss: 0.0342 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 6.6678e-04 - val_loss: 0.0329 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 6.2294e-04 - val_loss: 0.0334 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 5.9885e-04 - val_loss: 0.0321 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 5.5780e-04 - val_loss: 0.0316 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 5.2588e-04 - val_loss: 0.0284 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 5.0129e-04 - val_loss: 0.0279 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 4.8303e-04 - val_loss: 0.0269 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 4.7483e-04 - val_loss: 0.0271 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 4.4839e-04 - val_loss: 0.0303 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 4.2610e-04 - val_loss: 0.0281 - lr: 5.0000e-04\n",
      "Epoch 20/50\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 4.1333e-04 - val_loss: 0.0272 - lr: 5.0000e-04\n",
      "Epoch 21/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 4.0200e-04 - val_loss: 0.0261 - lr: 2.5000e-04\n",
      "Epoch 22/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 3.9704e-04 - val_loss: 0.0269 - lr: 2.5000e-04\n",
      "Epoch 23/50\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 3.9297e-04 - val_loss: 0.0264 - lr: 2.5000e-04\n",
      "Epoch 24/50\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 3.8880e-04 - val_loss: 0.0265 - lr: 1.2500e-04\n",
      "Epoch 25/50\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 3.8601e-04 - val_loss: 0.0259 - lr: 1.2500e-04\n",
      "Epoch 26/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 3.8463e-04 - val_loss: 0.0262 - lr: 1.2500e-04\n",
      "Epoch 27/50\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 3.8327e-04 - val_loss: 0.0257 - lr: 1.2500e-04\n",
      "Epoch 28/50\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 3.8164e-04 - val_loss: 0.0256 - lr: 1.2500e-04\n",
      "Epoch 29/50\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 3.8019e-04 - val_loss: 0.0257 - lr: 1.2500e-04\n",
      "Epoch 30/50\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 3.7744e-04 - val_loss: 0.0256 - lr: 6.2500e-05\n",
      "Epoch 31/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 3.7574e-04 - val_loss: 0.0257 - lr: 6.2500e-05\n",
      "Epoch 32/50\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 3.7449e-04 - val_loss: 0.0256 - lr: 3.1250e-05\n",
      "Epoch 33/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 3.7401e-04 - val_loss: 0.0259 - lr: 3.1250e-05\n",
      "Epoch 34/50\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 3.7345e-04 - val_loss: 0.0258 - lr: 3.1250e-05\n",
      "Epoch 35/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 3.7238e-04 - val_loss: 0.0258 - lr: 1.5625e-05\n",
      "Epoch 36/50\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 3.7214e-04 - val_loss: 0.0258 - lr: 1.5625e-05\n",
      "Epoch 37/50\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 3.7173e-04 - val_loss: 0.0258 - lr: 1.0000e-05\n",
      "Saved: stock_project/reports/predictions/gru_base.parquet | rows: 19536\n",
      "Saved: stock_project/reports/tables/gru_base_metrics.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>split</th>\n",
       "      <th>n</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Corr</th>\n",
       "      <th>DirectionalAcc</th>\n",
       "      <th>OOS_R2_vs_baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gru_base</td>\n",
       "      <td>train</td>\n",
       "      <td>7480</td>\n",
       "      <td>0.013821</td>\n",
       "      <td>0.019308</td>\n",
       "      <td>0.379172</td>\n",
       "      <td>0.595856</td>\n",
       "      <td>0.000886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gru_base</td>\n",
       "      <td>val</td>\n",
       "      <td>6048</td>\n",
       "      <td>0.138582</td>\n",
       "      <td>0.159862</td>\n",
       "      <td>0.043761</td>\n",
       "      <td>0.520503</td>\n",
       "      <td>-28.857380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gru_base</td>\n",
       "      <td>test</td>\n",
       "      <td>6008</td>\n",
       "      <td>0.104815</td>\n",
       "      <td>0.118774</td>\n",
       "      <td>-0.036799</td>\n",
       "      <td>0.541944</td>\n",
       "      <td>-21.972127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model  split     n       MAE      RMSE      Corr  DirectionalAcc  \\\n",
       "0  gru_base  train  7480  0.013821  0.019308  0.379172        0.595856   \n",
       "1  gru_base    val  6048  0.138582  0.159862  0.043761        0.520503   \n",
       "2  gru_base   test  6008  0.104815  0.118774 -0.036799        0.541944   \n",
       "\n",
       "   OOS_R2_vs_baseline  \n",
       "0            0.000886  \n",
       "1          -28.857380  \n",
       "2          -21.972127  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP 6.1 â€” GRU Baseline (NO GARCH)\n",
    "# - Uses the same numeric feature pool rule (Step 4)\n",
    "# - Trains on TRAIN, early-stops on VAL\n",
    "# - Saves predictions to reports/predictions/gru_base.parquet\n",
    "# - Writes metrics with matched OOS_RÂ² vs baseline_ticker_mean\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Preconditions\n",
    "assert \"panel\" in globals(), \"Run Step 2 first (load panel).\"\n",
    "assert \"feature_candidates_base\" in globals(), \"Run Step 4 first (feature candidates).\"\n",
    "assert \"make_sequences\" in globals(), \"Run Step 6 utilities first.\"\n",
    "assert \"build_gru\" in globals(), \"Run Step 6 utilities first.\"\n",
    "\n",
    "SEQ_LEN = 30\n",
    "FEATURE_COLS = feature_candidates_base  # consistent with ML baselines\n",
    "\n",
    "df = panel.copy()\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df[\"target_date\"] = pd.to_datetime(df[\"target_date\"])\n",
    "\n",
    "# Drop rows with missing split/target\n",
    "df = df.dropna(subset=[\"split\", \"target_ret\"])\n",
    "\n",
    "# Build sequences\n",
    "X, y, meta = make_sequences(df, FEATURE_COLS, seq_len=SEQ_LEN)\n",
    "print(\"X shape:\", X.shape, \"| y shape:\", y.shape, \"| meta:\", meta.shape)\n",
    "\n",
    "# Split indices using meta[\"split\"]\n",
    "idx_train = meta[\"split\"].values == \"train\"\n",
    "idx_val   = meta[\"split\"].values == \"val\"\n",
    "idx_test  = meta[\"split\"].values == \"test\"\n",
    "\n",
    "X_train, y_train = X[idx_train], y[idx_train]\n",
    "X_val, y_val     = X[idx_val], y[idx_val]\n",
    "X_test, y_test   = X[idx_test], y[idx_test]\n",
    "\n",
    "print(\"Train/Val/Test:\", X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "# Scale features (fit scaler on TRAIN only, apply to all)\n",
    "scaler = StandardScaler()\n",
    "F = X.shape[-1]\n",
    "\n",
    "X_train_2d = X_train.reshape(-1, F)\n",
    "scaler.fit(X_train_2d)\n",
    "\n",
    "def scale_X(X_in):\n",
    "    X2 = X_in.reshape(-1, F)\n",
    "    X2s = scaler.transform(X2)\n",
    "    return X2s.reshape(X_in.shape)\n",
    "\n",
    "X_train_s = scale_X(X_train)\n",
    "X_val_s   = scale_X(X_val)\n",
    "X_test_s  = scale_X(X_test)\n",
    "\n",
    "# Build & train model\n",
    "model = build_gru(input_shape=(SEQ_LEN, F))\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-5)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_s, y_train,\n",
    "    validation_data=(X_val_s, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=256,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Predict on all splits using the sequence meta\n",
    "yhat_train = model.predict(X_train_s, verbose=0).reshape(-1)\n",
    "yhat_val   = model.predict(X_val_s, verbose=0).reshape(-1)\n",
    "yhat_test  = model.predict(X_test_s, verbose=0).reshape(-1)\n",
    "\n",
    "# Build a single pred_df in your standard format\n",
    "pred_train = meta.loc[idx_train, [\"Date\",\"target_date\",\"Ticker\",\"split\"]].copy()\n",
    "pred_val   = meta.loc[idx_val,   [\"Date\",\"target_date\",\"Ticker\",\"split\"]].copy()\n",
    "pred_test  = meta.loc[idx_test,  [\"Date\",\"target_date\",\"Ticker\",\"split\"]].copy()\n",
    "\n",
    "pred_train[\"model\"] = \"gru_base\"\n",
    "pred_val[\"model\"]   = \"gru_base\"\n",
    "pred_test[\"model\"]  = \"gru_base\"\n",
    "\n",
    "pred_train[\"y_true\"] = y_train\n",
    "pred_val[\"y_true\"]   = y_val\n",
    "pred_test[\"y_true\"]  = y_test\n",
    "\n",
    "pred_train[\"y_pred\"] = yhat_train\n",
    "pred_val[\"y_pred\"]   = yhat_val\n",
    "pred_test[\"y_pred\"]  = yhat_test\n",
    "\n",
    "pred_df = pd.concat([pred_train, pred_val, pred_test], ignore_index=True)\n",
    "pred_df[\"residual\"] = pred_df[\"y_true\"] - pred_df[\"y_pred\"]\n",
    "\n",
    "# Save predictions\n",
    "pred_path = PRED_DIR / \"gru_base.parquet\"\n",
    "pred_df.to_parquet(pred_path, index=False)\n",
    "print(\"Saved:\", pred_path, \"| rows:\", len(pred_df))\n",
    "\n",
    "# Metrics table\n",
    "rows = []\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    d = pred_df[pred_df[\"split\"] == split]\n",
    "    m = regression_metrics(d[\"y_true\"].to_numpy(), d[\"y_pred\"].to_numpy())\n",
    "    rows.append({\"model\":\"gru_base\", \"split\": split, \"n\": int(len(d)), **m})\n",
    "metrics_df = pd.DataFrame(rows)\n",
    "\n",
    "# Matched OOS RÂ² vs baseline_ticker_mean\n",
    "baseline_path = PRED_DIR / \"baseline_ticker_mean.parquet\"\n",
    "oos = oos_r2_vs_baseline_matched(pred_df, baseline_path)\n",
    "metrics_df[\"OOS_R2_vs_baseline\"] = metrics_df[\"split\"].map(oos)\n",
    "\n",
    "# Save metrics\n",
    "out_path = TAB_DIR / \"gru_base_metrics.csv\"\n",
    "metrics_df.to_csv(out_path, index=False)\n",
    "print(\"Saved:\", out_path)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43bf8af-fc3c-4043-92ee-a5bc2840546e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
